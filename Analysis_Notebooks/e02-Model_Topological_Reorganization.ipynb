{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Initialize-Environment\" data-toc-modified-id=\"Initialize-Environment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initialize Environment</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-meta-data\" data-toc-modified-id=\"Load-meta-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load meta data</a></div><div class=\"lev1 toc-item\"><a href=\"#Measure-Evoked-Topology\" data-toc-modified-id=\"Measure-Evoked-Topology-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Measure Evoked Topology</a></div><div class=\"lev2 toc-item\"><a href=\"#Define-network-measurements\" data-toc-modified-id=\"Define-network-measurements-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Define network measurements</a></div><div class=\"lev2 toc-item\"><a href=\"#Baseline-Topology\" data-toc-modified-id=\"Baseline-Topology-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Baseline Topology</a></div><div class=\"lev2 toc-item\"><a href=\"#Stim-Topology\" data-toc-modified-id=\"Stim-Topology-33\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Stim Topology</a></div><div class=\"lev2 toc-item\"><a href=\"#Pre-analysis-Data-Handling\" data-toc-modified-id=\"Pre-analysis-Data-Handling-34\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Pre-analysis Data Handling</a></div><div class=\"lev1 toc-item\"><a href=\"#Modulation-of-Network-Topology\" data-toc-modified-id=\"Modulation-of-Network-Topology-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modulation of Network Topology</a></div><div class=\"lev2 toc-item\"><a href=\"#Compare-Stimulation-to-Baseline\" data-toc-modified-id=\"Compare-Stimulation-to-Baseline-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Compare Stimulation to Baseline</a></div><div class=\"lev3 toc-item\"><a href=\"#Pre-Post-Coherence\" data-toc-modified-id=\"Pre-Post-Coherence-411\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Pre-Post Coherence</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-Comparative-Distributions\" data-toc-modified-id=\"Plot-Comparative-Distributions-4111\"><span class=\"toc-item-num\">4.1.1.1&nbsp;&nbsp;</span>Plot Comparative Distributions</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-Modes-of-Functional-Control\" data-toc-modified-id=\"Plot-Modes-of-Functional-Control-4112\"><span class=\"toc-item-num\">4.1.1.2&nbsp;&nbsp;</span>Plot Modes of Functional Control</a></div><div class=\"lev2 toc-item\"><a href=\"#Compare-Stimulation-Energy\" data-toc-modified-id=\"Compare-Stimulation-Energy-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Compare Stimulation Energy</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-Stimulation-Energy-Grid\" data-toc-modified-id=\"Plot-Stimulation-Energy-Grid-421\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Plot Stimulation Energy Grid</a></div><div class=\"lev3 toc-item\"><a href=\"#Pre-Post-Coherence\" data-toc-modified-id=\"Pre-Post-Coherence-422\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Pre-Post Coherence</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-Comparative-Distributions\" data-toc-modified-id=\"Plot-Comparative-Distributions-4221\"><span class=\"toc-item-num\">4.2.2.1&nbsp;&nbsp;</span>Plot Comparative Distributions</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-Linear-Regression-for-Location\" data-toc-modified-id=\"Plot-Linear-Regression-for-Location-4222\"><span class=\"toc-item-num\">4.2.2.2&nbsp;&nbsp;</span>Plot Linear Regression for Location</a></div><div class=\"lev2 toc-item\"><a href=\"#Compare-Stimulation-Location\" data-toc-modified-id=\"Compare-Stimulation-Location-43\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Compare Stimulation Location</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-Stimulation-Locs-(MNI)\" data-toc-modified-id=\"Plot-Stimulation-Locs-(MNI)-431\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Plot Stimulation Locs (MNI)</a></div><div class=\"lev3 toc-item\"><a href=\"#Pre-Post-Coherence\" data-toc-modified-id=\"Pre-Post-Coherence-432\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Pre-Post Coherence</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-Comparative-Distributions\" data-toc-modified-id=\"Plot-Comparative-Distributions-4321\"><span class=\"toc-item-num\">4.3.2.1&nbsp;&nbsp;</span>Plot Comparative Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#Compare-Stimulation-to-Structural\" data-toc-modified-id=\"Compare-Stimulation-to-Structural-44\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Compare Stimulation to Structural</a></div><div class=\"lev3 toc-item\"><a href=\"#Pre-Post-Coherence\" data-toc-modified-id=\"Pre-Post-Coherence-441\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Pre-Post Coherence</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-Linear-Regression-for-Location\" data-toc-modified-id=\"Plot-Linear-Regression-for-Location-442\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>Plot Linear Regression for Location</a></div><div class=\"lev1 toc-item\"><a href=\"#Predicting-Node-Modulation\" data-toc-modified-id=\"Predicting-Node-Modulation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Predicting Node Modulation</a></div><div class=\"lev2 toc-item\"><a href=\"#Compare-Stimulation-to-Baseline\" data-toc-modified-id=\"Compare-Stimulation-to-Baseline-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Compare Stimulation to Baseline</a></div><div class=\"lev3 toc-item\"><a href=\"#Baseline-Coherence-with-Evoked-Nodes\" data-toc-modified-id=\"Baseline-Coherence-with-Evoked-Nodes-511\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Baseline Coherence with Evoked Nodes</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-Comparative-Distributions\" data-toc-modified-id=\"Plot-Comparative-Distributions-5111\"><span class=\"toc-item-num\">5.1.1.1&nbsp;&nbsp;</span>Plot Comparative Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#Compare-Stimulation-to-Structural\" data-toc-modified-id=\"Compare-Stimulation-to-Structural-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Compare Stimulation to Structural</a></div><div class=\"lev3 toc-item\"><a href=\"#Structural-Connectivity-with-Evoked-Nodes\" data-toc-modified-id=\"Structural-Connectivity-with-Evoked-Nodes-521\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Structural Connectivity with Evoked Nodes</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-Comparative-Distributions\" data-toc-modified-id=\"Plot-Comparative-Distributions-5211\"><span class=\"toc-item-num\">5.2.1.1&nbsp;&nbsp;</span>Plot Comparative Distributions</a></div><div class=\"lev1 toc-item\"><a href=\"#Relating-Structural-Control-and-Structural-Topology\" data-toc-modified-id=\"Relating-Structural-Control-and-Structural-Topology-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Relating Structural Control and Structural Topology</a></div><div class=\"lev2 toc-item\"><a href=\"#Controllability-and-connectivity-of-stim-location\" data-toc-modified-id=\"Controllability-and-connectivity-of-stim-location-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Controllability and connectivity of stim location</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-Linear-Regression\" data-toc-modified-id=\"Plot-Linear-Regression-611\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Plot Linear Regression</a></div><div class=\"lev1 toc-item\"><a href=\"#Modulation-Map\" data-toc-modified-id=\"Modulation-Map-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Modulation Map</a></div><div class=\"lev2 toc-item\"><a href=\"#Compute-the-map\" data-toc-modified-id=\"Compute-the-map-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Compute the map</a></div><div class=\"lev3 toc-item\"><a href=\"#Plot-the-Map\" data-toc-modified-id=\"Plot-the-Map-711\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>Plot the Map</a></div><div class=\"lev1 toc-item\"><a href=\"#Behavioral-States\" data-toc-modified-id=\"Behavioral-States-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Behavioral States</a></div><div class=\"lev1 toc-item\"><a href=\"#Memory-Classifier-States\" data-toc-modified-id=\"Memory-Classifier-States-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Memory Classifier States</a></div><div class=\"lev2 toc-item\"><a href=\"#Compare-Baseline-to-Memory-Classifier\" data-toc-modified-id=\"Compare-Baseline-to-Memory-Classifier-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Compare Baseline to Memory Classifier</a></div><div class=\"lev3 toc-item\"><a href=\"#Baseline-Node-Strength-with-Average-Memory-Classifier\" data-toc-modified-id=\"Baseline-Node-Strength-with-Average-Memory-Classifier-911\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>Baseline Node Strength with Average Memory Classifier</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-Comparative-Distributions\" data-toc-modified-id=\"Plot-Comparative-Distributions-9111\"><span class=\"toc-item-num\">9.1.1.1&nbsp;&nbsp;</span>Plot Comparative Distributions</a></div><div class=\"lev2 toc-item\"><a href=\"#Compare-Structural-Control-to-Memory-Classifier\" data-toc-modified-id=\"Compare-Structural-Control-to-Memory-Classifier-92\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Compare Structural Control to Memory Classifier</a></div><div class=\"lev3 toc-item\"><a href=\"#Controllability-with-Average-Memory-Classifier\" data-toc-modified-id=\"Controllability-with-Average-Memory-Classifier-921\"><span class=\"toc-item-num\">9.2.1&nbsp;&nbsp;</span>Controllability with Average Memory Classifier</a></div><div class=\"lev4 toc-item\"><a href=\"#Plot-Linear-Regression-for-Location\" data-toc-modified-id=\"Plot-Linear-Regression-for-Location-9211\"><span class=\"toc-item-num\">9.2.1.1&nbsp;&nbsp;</span>Plot Linear Regression for Location</a></div><div class=\"lev2 toc-item\"><a href=\"#Modal-Controllability-Distribution-(MNI)\" data-toc-modified-id=\"Modal-Controllability-Distribution-(MNI)-93\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Modal Controllability Distribution (MNI)</a></div><div class=\"lev1 toc-item\"><a href=\"#Artificial-Energy-Landscape\" data-toc-modified-id=\"Artificial-Energy-Landscape-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Artificial Energy Landscape</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %reset\n",
    "except:\n",
    "    print 'NOT IPYTHON'\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "def write(txt):\n",
    "    sys.stdout.write(txt)\n",
    "    sys.stdout.flush()\n",
    "nb_stdout = sys.stdout    \n",
    "\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "import scipy.linalg as scialg\n",
    "import scipy.stats as stats\n",
    "import scipy.io as io\n",
    "import h5py\n",
    "import pickle as pkl\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import structural_control\n",
    "sys.path.append('/Users/akhambhati/Developer/hoth_research/Echobase')\n",
    "import Echobase\n",
    "convert_adj_matr_to_cfg_matr = Echobase.Network.Transforms.configuration.convert_adj_matr_to_cfg_matr\n",
    "convert_conn_vec_to_adj_matr = Echobase.Network.Transforms.configuration.convert_conn_vec_to_adj_matr\n",
    "\n",
    "#rcParams = Echobase.Plotting.fig_format.update_rcparams(rcParams)\n",
    "#rcParams.update(rcParams)\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "matplotlib.rc(\"font\", family=\"Helvetica\")\n",
    "\n",
    "path_CoreData = '/Users/akhambhati/Remotes/CORE.PS_Stim'\n",
    "path_PeriphData = '/Users/akhambhati/Remotes/RSRCH.PS_Stim'\n",
    "path_AtlasData = '/Users/akhambhati/Remotes/CORE.MRI_Atlases'\n",
    "\n",
    "path_Data = {'Input_Meta': {},\n",
    "             'Input_Stim': {},\n",
    "             'Input_Baseline': {},\n",
    "             'Output': {},\n",
    "             'Figure': {}}\n",
    "path_Data['Input_Meta'] = path_PeriphData + '/e00-Multimodal_Mapping'\n",
    "path_Data['Input_Stim'] = path_PeriphData + '/e02-FuncNetw.CommonAverage.Stim'\n",
    "path_Data['Input_Baseline'] = path_PeriphData + '/e02-FuncNetw.CommonAverage.Baseline'\n",
    "path_Data['Output'] = path_PeriphData + '/e02-GlobalTopo.CommonAverage'\n",
    "path_Data['Output'] = path_PeriphData + '/e02-GlobalTopo.CommonAverage'\n",
    "path_Data['Figure'] = './Figures-e02-GlobalTopo.CommonAverage'\n",
    "\n",
    "for path_type in path_Data.keys():\n",
    "    path = path_Data[path_type]\n",
    "    if not os.path.exists(path_Data[path_type]):\n",
    "        print('Path: {}, does not exist'.format(path))\n",
    "        os.makedirs(path)\n",
    "            \n",
    "from sklearn.externals.joblib import Memory\n",
    "memory = Memory(cachedir=path_Data['Output'], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meta_dict = {}\n",
    "\n",
    "meta_dict['subject_list'] = np.load('{}/Meta.Electrode_Loc.npz'.format(path_Data['Input_Meta']))['subj_list']\n",
    "meta_dict['electrode_loc'] = np.load('{}/Meta.Electrode_Loc.npz'.format(path_Data['Input_Meta']))['channel_info'][()]\n",
    "meta_dict['atlas_info'] = np.load('{}/Meta.Electrode_Loc.npz'.format(path_Data['Input_Meta']))['lausanne_label_mni'][()]\n",
    "meta_dict['struct_adj'] = np.load('{}/Meta.Electrode_Loc.npz'.format(path_Data['Input_Meta']))['structadj_dict'][()]\n",
    "meta_dict['memory_info'] = pd.read_pickle('{}/Meta.Memory_Info.pkl'.format(path_Data['Input_Meta']))\n",
    "meta_dict['baseline_info'] = pd.read_pickle('{}/Meta.Baseline_Info.pkl'.format(path_Data['Input_Meta']))\n",
    "meta_dict['stim_info'] = pd.read_pickle('{}/Meta.Stim_Info.pkl'.format(path_Data['Input_Meta']))\n",
    "meta_dict['behavior_info'] = pd.read_pickle('{}/Meta.Behavior_Info.pkl'.format(path_Data['Input_Meta']))\n",
    "meta_dict['LTC_info'] = ['inferiortemporal', 'middletemporal', 'superiortemporal',\n",
    "                         'temporalpole', 'parahippocampal', 'entorhinal', 'lingual',\n",
    "                         'fusiform','transversetemporal', 'bankssts']\n",
    "\n",
    "meta_dict['coherence_info'] = ['AlphaTheta', 'Beta', 'LowGamma', 'HighGamma']\n",
    "meta_dict['coherence_freq'] = ['5-15 Hz', '15-25 Hz', '30-40 Hz', '95-105 Hz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Evoked Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cohens_d(dist1, dist2):\n",
    "    m1 = np.mean(dist1)\n",
    "    m2 = np.mean(dist2)\n",
    "    s12 = np.std(np.concatenate((dist1, dist2)).reshape(-1))\n",
    "    return (m1-m2)/s12\n",
    "\n",
    "\n",
    "def measure_cfg_str(adj):\n",
    "    cfg_vec = convert_adj_matr_to_cfg_matr(np.expand_dims(adj, axis=0))[0, :]\n",
    "    cfg_mean = np.mean(cfg_vec)\n",
    "    cfg_var = np.var(cfg_vec)\n",
    "    return cfg_mean, cfg_var\n",
    "\n",
    "\n",
    "def measure_node_str(adj):\n",
    "    node_str = np.nanmean(adj, axis=0)\n",
    "    node_str_z = (node_str - np.nanmean(node_str)) / np.nanstd(node_str)    \n",
    "    return node_str, node_str_z\n",
    "\n",
    "\n",
    "def measure_node_str_delta(adj_pre, adj_post, n_perm=1000):\n",
    "    cfg_pre = convert_adj_matr_to_cfg_matr(np.expand_dims(adj_pre, axis=0))[0, :]\n",
    "    cfg_post = convert_adj_matr_to_cfg_matr(np.expand_dims(adj_post, axis=0))[0, :]\n",
    "    cfg_delta = cfg_post - cfg_pre\n",
    "    \n",
    "    # True delta node_str\n",
    "    node_str_delta = measure_node_str(convert_conn_vec_to_adj_matr(cfg_delta))[0]\n",
    "    \n",
    "    # Permutation test for delta node_str\n",
    "    node_str_delta_null = np.array([\n",
    "        measure_node_str(\n",
    "            convert_conn_vec_to_adj_matr(\n",
    "                np.random.permutation(cfg_delta)))[0]\n",
    "        for p_ii in xrange(n_perm)])\n",
    "    node_str_delta_pv = np.mean(np.abs(node_str_delta_null) > np.abs(node_str_delta), axis=0)\n",
    "    \n",
    "    return node_str_delta, node_str_delta_pv\n",
    "\n",
    "\n",
    "def measure_cfg_sim(adj_pre, adj_post):\n",
    "    cfg_pre = convert_adj_matr_to_cfg_matr(np.expand_dims(adj_pre, axis=0))[0, :]\n",
    "    cfg_post = convert_adj_matr_to_cfg_matr(np.expand_dims(adj_post, axis=0))[0, :]\n",
    "    cfg_sim = np.corrcoef(cfg_pre, cfg_post)[0, 1]\n",
    "    return cfg_sim\n",
    "\n",
    "\n",
    "def measure_dist_node_corr(node_str_del, dist, stim_ix):\n",
    "    dist_to_stim = np.mean(dist[stim_ix, :], axis=0)\n",
    "    dist_to_stim = np.delete(dist_to_stim, stim_ix)\n",
    "    rv, pv = stats.pearsonr(dist_to_stim, node_str_del)\n",
    "    \n",
    "    return rv, pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fdf['adj'][()]['LIHG3_LIHG4'][500][2]['Post_Stim_1']['Beta']\n",
    "# Structure of baseline files\n",
    "\n",
    "def generate_globaltopo_baseline(proc_id):\n",
    "    print(proc_id)\n",
    "    meta_base = meta_dict['baseline_info'].ix[proc_id]\n",
    "\n",
    "    subj_id = meta_base['Subject_ID']\n",
    "    base_id = meta_base['Base_ID'] \n",
    "    path_input_base = path_Data['Input_Baseline']    \n",
    "    prev_n_win = 60\n",
    "    \n",
    "    write('{}-{}\\n'.format(subj_id, base_id))\n",
    "    \n",
    "    data_table = {'Subject_ID': [],\n",
    "                  'Base_ID': [],\n",
    "                  'Coherence_ID': [],\n",
    "                  'Stim_Anode': [],\n",
    "                  'Stim_Cathode': [],\n",
    "                  'Stim_Duration': [],\n",
    "                  \n",
    "                  'Pre_Cfg_Str_Mean': [],\n",
    "                  'Pre_Cfg_Str_Var': [],\n",
    "                  'Post1_Cfg_Str_Mean': [],\n",
    "                  'Post1_Cfg_Str_Var': [],\n",
    "                  'Post2_Cfg_Str_Mean': [],\n",
    "                  'Post2_Cfg_Str_Var': [],\n",
    "                  \n",
    "                  'Pre_Node_Str': [],\n",
    "                  'Pre_Node_Str_Z': [],                  \n",
    "                  'Post1_Node_Str': [],\n",
    "                  'Post1_Node_Str_Z': [],                  \n",
    "                  'Post2_Node_Str': [],\n",
    "                  'Post2_Node_Str_Z': [],                  \n",
    "                  \n",
    "                  'Stim_Node_Str_Delta1': [],\n",
    "                  'Stim_Node_Str_Delta1_pv': [],\n",
    "                  'Stim_Node_Str_Delta2': [],\n",
    "                  'Stim_Node_Str_Delta2_pv': [],\n",
    "                                    \n",
    "                  'Stim_Cfg_Sim1': [],\n",
    "                  'Stim_Cfg_Sim2': [],   \n",
    "                  \n",
    "                  'Mean_Adj': []}\n",
    "    \n",
    "    # Load \n",
    "    df_base = np.load('{}/Adjacency.{}.Base_Event.{}.npz'.format(path_input_base, subj_id, base_id))\n",
    "\n",
    "    for stim_pair_tag in df_base['adj'][()].keys():\n",
    "        for stim_duration in df_base['adj'][()][stim_pair_tag].keys():\n",
    "            for ep_ii, ep in enumerate(df_base['adj'][()][stim_pair_tag][stim_duration][-prev_n_win:]):\n",
    "                for coh_id in meta_dict['coherence_info']:\n",
    "\n",
    "                    try:\n",
    "                        adj_pre = ep['Pre_Stim'][coh_id]\n",
    "                    except:\n",
    "                        continue\n",
    "                    N, N = adj_pre.shape\n",
    "\n",
    "                    try:\n",
    "                        adj_post1 = ep['Post_Stim_1'][coh_id]\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        adj_post2 = ep['Post_Stim_2'][coh_id]\n",
    "                    except:\n",
    "                        adj_post2 = np.nan*np.zeros((N, N))\n",
    "\n",
    "                    # Add to Data Table                      \n",
    "                    data_table['Subject_ID'].append(subj_id)\n",
    "                    data_table['Base_ID'].append(ep_ii)\n",
    "                    data_table['Coherence_ID'].append(coh_id)\n",
    "                    data_table['Stim_Anode'].append(stim_pair_tag.split('_')[0])\n",
    "                    data_table['Stim_Cathode'].append(stim_pair_tag.split('_')[1])\n",
    "                    data_table['Stim_Duration'].append(stim_duration)\n",
    "\n",
    "                    data_table['Pre_Cfg_Str_Mean'].append(measure_cfg_str(adj_pre)[0])\n",
    "                    data_table['Pre_Cfg_Str_Var'].append(measure_cfg_str(adj_pre)[1])\n",
    "                    data_table['Post1_Cfg_Str_Mean'].append(measure_cfg_str(adj_post1)[0])\n",
    "                    data_table['Post1_Cfg_Str_Var'].append(measure_cfg_str(adj_post1)[1])\n",
    "                    data_table['Post2_Cfg_Str_Mean'].append(measure_cfg_str(adj_post2)[0])\n",
    "                    data_table['Post2_Cfg_Str_Var'].append(measure_cfg_str(adj_post2)[1])\n",
    "\n",
    "                    data_table['Pre_Node_Str'].append(measure_node_str(adj_pre)[0])\n",
    "                    data_table['Pre_Node_Str_Z'].append(measure_node_str(adj_pre)[1]) \n",
    "                    data_table['Post1_Node_Str'].append(measure_node_str(adj_post1)[0])\n",
    "                    data_table['Post1_Node_Str_Z'].append(measure_node_str(adj_post1)[1]) \n",
    "                    data_table['Post2_Node_Str'].append(measure_node_str(adj_post2)[0])\n",
    "                    data_table['Post2_Node_Str_Z'].append(measure_node_str(adj_post2)[1])\n",
    "\n",
    "                    ns_del = measure_node_str_delta(adj_pre, adj_post1)\n",
    "                    data_table['Stim_Node_Str_Delta1'].append(ns_del[0])\n",
    "                    data_table['Stim_Node_Str_Delta1_pv'].append(ns_del[1])                        \n",
    "\n",
    "                    ns_del = measure_node_str_delta(adj_pre, adj_post2)\n",
    "                    data_table['Stim_Node_Str_Delta2'].append(ns_del[0])\n",
    "                    data_table['Stim_Node_Str_Delta2_pv'].append(ns_del[1])                        \n",
    "\n",
    "                    cfg_sim = measure_cfg_sim(adj_pre, adj_post1)\n",
    "                    data_table['Stim_Cfg_Sim1'].append(cfg_sim)\n",
    "\n",
    "                    cfg_sim = measure_cfg_sim(adj_pre, adj_post2)\n",
    "                    data_table['Stim_Cfg_Sim2'].append(cfg_sim)\n",
    "\n",
    "                    data_table['Mean_Adj'].append(adj_pre)                        \n",
    "\n",
    "    globaltopo_base = pd.DataFrame(data_table, columns=data_table.keys())\n",
    "    return globaltopo_base\n",
    "\n",
    "################################\n",
    "output_path = '{}/GlobalTopo.Baseline.pkl'.format(path_Data['Output'])\n",
    "if not os.path.exists(output_path):\n",
    "    sys.stdout = open('{}/GlobalTopo.Baseline.ProcJob'.format(path_Data['Output']), 'w')    \n",
    "    write('Mapping jobs...\\n')\n",
    "    \n",
    "    args_list = range(len(meta_dict['baseline_info']))\n",
    "    #res = map(generate_globaltopo_baseline, args_list)\n",
    "    pool = Pool(7)\n",
    "    res = pool.map(generate_globaltopo_baseline, args_list)\n",
    "\n",
    "\n",
    "    write('\\nReducing jobs...\\n')\n",
    "    df_globaltopo_base = pd.concat(res, ignore_index=True)\n",
    "    df_globaltopo_base.to_pickle(output_path)\n",
    "    sys.stdout = nb_stdout\n",
    "else:\n",
    "    df_globaltopo_base = pd.read_pickle(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stim Topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_globaltopo_stim(proc_id):\n",
    "    meta_stim = meta_dict['stim_info'].ix[proc_id]\n",
    "    \n",
    "    subj_id = meta_stim['Subject_ID']\n",
    "    event_id = meta_stim['Event_ID']    \n",
    "    path_input_stim = path_Data['Input_Stim']\n",
    "    \n",
    "    write('{}-{}\\n'.format(subj_id, event_id))    \n",
    "    \n",
    "    data_table = {'Subject_ID': [],\n",
    "                  'Event_ID': [],\n",
    "                  'Coherence_ID': [],\n",
    "                  \n",
    "                  'Pre_Cfg_Str_Mean': [],\n",
    "                  'Pre_Cfg_Str_Var': [],\n",
    "                  'Post1_Cfg_Str_Mean': [],\n",
    "                  'Post1_Cfg_Str_Var': [],\n",
    "                  'Post2_Cfg_Str_Mean': [],\n",
    "                  'Post2_Cfg_Str_Var': [],\n",
    "                  \n",
    "                  'Pre_Node_Str': [],\n",
    "                  'Pre_Node_Str_Z': [],                  \n",
    "                  'Post1_Node_Str': [],\n",
    "                  'Post1_Node_Str_Z': [],                  \n",
    "                  'Post2_Node_Str': [],\n",
    "                  'Post2_Node_Str_Z': [],                  \n",
    "                  \n",
    "                  'Stim_Node_Str_Delta1': [],\n",
    "                  'Stim_Node_Str_Delta1_pv': [],\n",
    "                  'Stim_Node_Str_Delta2': [],\n",
    "                  'Stim_Node_Str_Delta2_pv': [],\n",
    "                                    \n",
    "                  'Stim_Cfg_Sim1': [],\n",
    "                  'Stim_Cfg_Sim2': []}\n",
    "    \n",
    "    # Load\n",
    "    try:\n",
    "        df_stim = np.load('{}/Adjacency.{}.Stim_Event.{}.npz'.format(path_input_stim, subj_id, event_id))\n",
    "    except IOError:\n",
    "        globaltopo_stim = pd.DataFrame(data_table, columns=data_table.keys())\n",
    "        return globaltopo_stim\n",
    "    \n",
    "    for coh_id in meta_dict['coherence_info']:\n",
    "\n",
    "        try:\n",
    "            adj_pre = df_stim['adj'][()]['Pre_Stim'][coh_id]\n",
    "        except:\n",
    "            continue\n",
    "        N, N = adj_pre.shape\n",
    "\n",
    "        try:\n",
    "            adj_post1 = df_stim['adj'][()]['Post_Stim_1'][coh_id]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            adj_post2 = df_stim['adj'][()]['Post_Stim_2'][coh_id]\n",
    "        except:\n",
    "            adj_post2 = np.nan*np.zeros((N, N))\n",
    "\n",
    "        # Add to Data Table                      \n",
    "        data_table['Subject_ID'].append(subj_id)\n",
    "        data_table['Event_ID'].append(event_id)\n",
    "        data_table['Coherence_ID'].append(coh_id)\n",
    "\n",
    "        data_table['Pre_Cfg_Str_Mean'].append(measure_cfg_str(adj_pre)[0])\n",
    "        data_table['Pre_Cfg_Str_Var'].append(measure_cfg_str(adj_pre)[1])\n",
    "        data_table['Post1_Cfg_Str_Mean'].append(measure_cfg_str(adj_post1)[0])\n",
    "        data_table['Post1_Cfg_Str_Var'].append(measure_cfg_str(adj_post1)[1])\n",
    "        data_table['Post2_Cfg_Str_Mean'].append(measure_cfg_str(adj_post2)[0])\n",
    "        data_table['Post2_Cfg_Str_Var'].append(measure_cfg_str(adj_post2)[1])\n",
    "\n",
    "        data_table['Pre_Node_Str'].append(measure_node_str(adj_pre)[0])\n",
    "        data_table['Pre_Node_Str_Z'].append(measure_node_str(adj_pre)[1]) \n",
    "        data_table['Post1_Node_Str'].append(measure_node_str(adj_post1)[0])\n",
    "        data_table['Post1_Node_Str_Z'].append(measure_node_str(adj_post1)[1]) \n",
    "        data_table['Post2_Node_Str'].append(measure_node_str(adj_post2)[0])\n",
    "        data_table['Post2_Node_Str_Z'].append(measure_node_str(adj_post2)[1])\n",
    "\n",
    "        ns_del = measure_node_str_delta(adj_pre, adj_post1)\n",
    "        data_table['Stim_Node_Str_Delta1'].append(ns_del[0])\n",
    "        data_table['Stim_Node_Str_Delta1_pv'].append(ns_del[1])                        \n",
    "\n",
    "        ns_del = measure_node_str_delta(adj_pre, adj_post2)\n",
    "        data_table['Stim_Node_Str_Delta2'].append(ns_del[0])\n",
    "        data_table['Stim_Node_Str_Delta2_pv'].append(ns_del[1])                        \n",
    "\n",
    "        cfg_sim = measure_cfg_sim(adj_pre, adj_post1)\n",
    "        data_table['Stim_Cfg_Sim1'].append(cfg_sim)\n",
    "\n",
    "        cfg_sim = measure_cfg_sim(adj_pre, adj_post2)\n",
    "        data_table['Stim_Cfg_Sim2'].append(cfg_sim)                     \n",
    "            \n",
    "    globaltopo_stim = pd.DataFrame(data_table, columns=data_table.keys())\n",
    "    return globaltopo_stim\n",
    "\n",
    "################################\n",
    "output_path = '{}/GlobalTopo.Stimulation.pkl'.format(path_Data['Output'])\n",
    "if not os.path.exists(output_path):\n",
    "    sys.stdout = open('{}/GlobalTopo.Stimulation.ProcJob'.format(path_Data['Output']), 'w')\n",
    "    write('Mapping jobs...\\n')\n",
    "\n",
    "    args_list = range(len(meta_dict['stim_info']))\n",
    "    pool = Pool(7)\n",
    "    res = pool.map(generate_globaltopo_stim, args_list)\n",
    "    \n",
    "    write('\\nReducing jobs...\\n')\n",
    "    df_globaltopo_stim = pd.concat(res, ignore_index=True)\n",
    "    df_globaltopo_stim.to_pickle(output_path)\n",
    "    write('Done.\\n')\n",
    "    sys.stdout = nb_stdout\n",
    "else:\n",
    "    df_globaltopo_stim = pd.read_pickle(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-analysis Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First merge df_globaltopo_stim with meta_ps\n",
    "df_globaltopo_stim = pd.merge(meta_dict['stim_info'],\n",
    "                              df_globaltopo_stim,                              \n",
    "                              on=['Subject_ID', 'Event_ID'])\n",
    "\n",
    "# Second retain only stimulating types\n",
    "df_globaltopo_stim = df_globaltopo_stim.loc[df_globaltopo_stim.Stim_Type == 'stimulating']\n",
    "\n",
    "# Third discard PS0\n",
    "df_globaltopo_stim = df_globaltopo_stim[df_globaltopo_stim['Experiment_ID'] != 'PS0']\n",
    "#df_globaltopo_stim = df_globaltopo_stim[df_globaltopo_stim['Experiment_ID'].isin(['PS1', 'PS2', 'PS2.1'])]\n",
    "\n",
    "# Get common subjects\n",
    "common_subject_list = np.intersect1d(np.unique(df_globaltopo_base['Subject_ID']),\n",
    "                                     np.unique(df_globaltopo_stim['Subject_ID']))\n",
    "\n",
    "#### Get bad subjects (fix preproc)\n",
    "good_subj = []\n",
    "bad_subj = []\n",
    "for subj_id in meta_dict['electrode_loc'].keys():\n",
    "    if len(meta_dict['electrode_loc'][subj_id]['Monopolar']['id']) == len(meta_dict['electrode_loc'][subj_id]['Monopolar']['lbl']):\n",
    "        good_subj.append(subj_id)\n",
    "    else:\n",
    "        bad_subj.append(subj_id)\n",
    "good_subj, bad_subj\n",
    "common_subject_list = np.setdiff1d(common_subject_list, bad_subj)\n",
    "\n",
    "# Reduce the DFs for common subjects\n",
    "df_globaltopo_stim = df_globaltopo_stim.loc[df_globaltopo_stim.Subject_ID.isin(common_subject_list)]\n",
    "df_globaltopo_base = df_globaltopo_base.loc[df_globaltopo_base.Subject_ID.isin(common_subject_list)]\n",
    "\n",
    "# Make sure Base and Stim have common column names\n",
    "df_globaltopo_base = df_globaltopo_base.rename(\n",
    "    columns={'Stim_Duration': 'Stim_Dur'})\n",
    "print('Common Column Fields')\n",
    "common_cols = list(np.intersect1d(df_globaltopo_base.keys(),\n",
    "                                  df_globaltopo_stim.keys()))\n",
    "print(common_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulation of Network Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Stimulation to Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key = ['Subject_ID',\n",
    "                   'Stim_Anode',\n",
    "                   'Stim_Cathode',\n",
    "                   'Stim_Dur',\n",
    "                   'Coherence_ID']\n",
    "\n",
    "df_stim_sel = df_globaltopo_stim.groupby(unique_stim_key).indices\n",
    "df_base_sel = df_globaltopo_base.groupby(unique_stim_key).indices\n",
    "\n",
    "# Use Keys that overlap between stim and base\n",
    "df_base_key = df_base_sel.keys()\n",
    "df_stim_key = df_stim_sel.keys()\n",
    "df_base_key_set = np.array([set(k) for k in df_base_key])\n",
    "df_stim_key_set = np.array([set(k) for k in df_stim_key])\n",
    "\n",
    "df_base_common_key = []\n",
    "df_stim_common_key = []\n",
    "for b_k_ii, b_k in enumerate(df_base_key_set):\n",
    "    for s_k_ii, s_k in enumerate(df_stim_key_set):\n",
    "        if b_k == s_k:\n",
    "            df_base_common_key.append(df_base_key[b_k_ii])\n",
    "            df_stim_common_key.append(df_stim_key[s_k_ii])            \n",
    "            continue\n",
    "            \n",
    "df_luts = {'stim': {'common_key': df_stim_common_key,\n",
    "                    'df_ind': df_stim_sel,\n",
    "                    'df_topo': df_globaltopo_stim},\n",
    "           'base': {'common_key': df_base_common_key,\n",
    "                    'df_ind': df_base_sel,\n",
    "                    'df_topo': df_globaltopo_base}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Post Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@memory.cache\n",
    "def compute_prepost_coherence(cfg_measure):\n",
    "    print('Processing: {}'.format(cfg_measure))\n",
    "        \n",
    "    # Initialize the measurement buckets (2 columns for post1/post2)\n",
    "    vals = {'Subject_ID': [],\n",
    "            'Coherence_ID': [],\n",
    "            'Stim_Type': [],\n",
    "            'Stim_Loc': [],\n",
    "            cfg_measure + '_Del_1': [],\n",
    "            cfg_measure + '_Del_2': []}\n",
    "    \n",
    "    # Condence the measurement across the common key criteria\n",
    "    for tp in df_luts.keys():\n",
    "        df_key = df_luts[tp]['common_key']\n",
    "        df_ind = df_luts[tp]['df_ind']        \n",
    "        df_topo = df_luts[tp]['df_topo']\n",
    "    \n",
    "        # Iterate over keys\n",
    "        for key_ii, key in enumerate(df_key):\n",
    "            df_sel = df_topo.iloc[df_ind[key]]\n",
    "\n",
    "            subj_id = df_sel['Subject_ID'].iloc[0]\n",
    "            coh_id = df_sel['Coherence_ID'].iloc[0]\n",
    "            stim_anode = df_sel['Stim_Anode'].iloc[0]\n",
    "            stim_cathode = df_sel['Stim_Cathode'].iloc[0]\n",
    "            \n",
    "            # Compute Measurement\n",
    "            if cfg_measure in ['Cfg_Str_Mean', 'Cfg_Str_Var']:\n",
    "                post1_vs_pre = (df_sel['Post1_' + cfg_measure] - df_sel['Pre_' + cfg_measure]).mean()\n",
    "                post2_vs_pre = (df_sel['Post2_' + cfg_measure] - df_sel['Pre_' + cfg_measure]).mean()\n",
    "            elif cfg_measure in ['Stim_Cfg_Sim']:\n",
    "                post1_vs_pre = (df_sel[cfg_measure + '1']).mean()\n",
    "                post2_vs_pre = (df_sel[cfg_measure + '2']).mean()\n",
    "            elif cfg_measure in ['Stim_Node_Str_Delta']:\n",
    "                post1_vs_pre = np.mean([np.nanmean(ev < (0.05)/len(ev))\n",
    "                                        for ev in df_sel[cfg_measure + '1_pv']])\n",
    "                post2_vs_pre = np.mean([np.nanmean(ev < (0.05)/len(ev))\n",
    "                                        for ev in df_sel[cfg_measure + '2_pv']])\n",
    "            else:\n",
    "                raise ValueError('{} not a valid measurement'.format(cfg_measure))\n",
    "\n",
    "            # Add to the dictionary\n",
    "            vals['Subject_ID'].append(subj_id)\n",
    "            vals['Coherence_ID'].append(coh_id)\n",
    "            vals['Stim_Type'].append(tp)\n",
    "            vals['Stim_Loc'].append('_'.join(np.sort([stim_anode, stim_cathode])))\n",
    "            vals[cfg_measure + '_Del_1'].append(post1_vs_pre)\n",
    "            vals[cfg_measure + '_Del_2'].append(post2_vs_pre)            \n",
    "    \n",
    "    # Convert to pandas dataframe\n",
    "    df = pd.DataFrame(vals, columns=vals.keys())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Comparative Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Cfg_Str_Mean':          {'xlabel': 'Frequency Range',\n",
    "                                      'ylabel': 'Delta Mean Coherence'},\n",
    "            'Cfg_Str_Var':           {'xlabel': 'Frequency Range',\n",
    "                                      'ylabel': 'Delta Var Coherence'},            \n",
    "            'Stim_Cfg_Sim':          {'xlabel': 'Frequency Range',\n",
    "                                      'ylabel': 'Edge Correlation'}}                             \n",
    "\n",
    "for meas in analysis.keys():\n",
    "    df = compute_prepost_coherence(meas)    \n",
    "    df = df.groupby(['Subject_ID', 'Coherence_ID', 'Stim_Type']).mean().reset_index()\n",
    "    \n",
    "    # Get basic trends/stats\n",
    "    df_range_median = df.groupby(['Coherence_ID', 'Stim_Type']).median().reset_index()\n",
    "    df_range_q1 = df.groupby(['Coherence_ID', 'Stim_Type']).quantile(0.25).reset_index()\n",
    "    df_range_q3 = df.groupby(['Coherence_ID', 'Stim_Type']).quantile(0.75).reset_index()\n",
    "\n",
    "    # Generate plots and stats\n",
    "    for delay_prefix in ['_Del_1', '_Del_2']:\n",
    "        meas_full = meas + delay_prefix\n",
    "        \n",
    "        # Compute window ranges\n",
    "        iqr = (df_range_q3[meas_full]-df_range_q1[meas_full]).max()\n",
    "        iqr_fac = 1.0\n",
    "        max_ylim = df_range_q3[meas_full].max() + iqr_fac*iqr\n",
    "        min_ylim = df_range_q1[meas_full].min() - iqr_fac*iqr\n",
    "                \n",
    "        # Seaborn params\n",
    "        plt.figure()\n",
    "        ax = plt.subplot(111)        \n",
    "        sns_plot_params = {'x': 'Coherence_ID',\n",
    "                           'y': meas_full,\n",
    "                           'order': meta_dict['coherence_info'],\n",
    "                           'hue': 'Stim_Type',\n",
    "                           'hue_order': ['stim', 'base'],\n",
    "                           'palette': 'Set1',\n",
    "                           'data': df,\n",
    "                           'ax': ax}\n",
    "        lw = 2.0\n",
    "        # Add a box\n",
    "        ax = sns.boxplot(showfliers=False,\n",
    "                         whis=0.0,\n",
    "                         medianprops={'linewidth': lw},\n",
    "                         **sns_plot_params)\n",
    "        # Remove facecolor from boxes\n",
    "        for ii, artist in enumerate(ax.artists):\n",
    "            col = artist.get_facecolor()\n",
    "            artist.set_edgecolor([0,0,0,0])\n",
    "            artist.set_facecolor([0,0,0,0])\n",
    "        for ii, lines in enumerate(ax.lines):\n",
    "            lines.set_linewidth(lw)\n",
    "        # Add a strip plot points            \n",
    "        ax = sns.stripplot(dodge=True,\n",
    "                           jitter=True,\n",
    "                           size=2,\n",
    "                           alpha=0.5,\n",
    "                           **sns_plot_params)       \n",
    "        # Remove legend\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        l = plt.legend(handles[-2:], labels[-2:],\n",
    "                       bbox_to_anchor=(1, 1), loc=2,\n",
    "                       borderaxespad=0.)\n",
    "        \n",
    "        # Prettify the plotting + add labels\n",
    "        ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "        ax.set_ylabel(analysis[meas]['ylabel'])   \n",
    "        if meas == 'Stim_Cfg_Sim':\n",
    "            ax.set_ylim([0.0, 1.0])\n",
    "        if meas == 'Cfg_Str_Mean':\n",
    "            ax.set_ylim([-0.02, 0.02])\n",
    "        ax.set_title(meas_full)\n",
    "        sns.despine(ax=ax)\n",
    "        sns.set_context('paper')\n",
    "\n",
    "        plt.savefig('{}/Delta-{}.BarPlot.svg'.format(path_Data['Figure'], 'example'))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        \n",
    "        # Stats\n",
    "        write(':: STATS ::\\n')\n",
    "        df_A = df[df['Stim_Type'] == 'base']\n",
    "        df_B = df[df['Stim_Type'] == 'stim']\n",
    "\n",
    "        A_arr = []\n",
    "        B_arr = []\n",
    "        for coh_id in meta_dict['coherence_info']:\n",
    "            A = df_A[df_A['Coherence_ID'] == coh_id][meas_full]\n",
    "            B = df_B[df_B['Coherence_ID'] == coh_id][meas_full]  \n",
    "            mask = np.logical_and(np.isfinite(A), np.isfinite(B))\n",
    "            A = A[np.array(mask)]\n",
    "            B = B[np.array(mask)]\n",
    "            \n",
    "            A_arr.append(A)\n",
    "            B_arr.append(B)            \n",
    "            \n",
    "            write('{}\\n'.format(coh_id))\n",
    "            write('    Shapiro      :: (Base)      --> {}\\n'.format(stats.shapiro(A)))\n",
    "            write('    Shapiro      :: (Stim)      --> {}\\n'.format(stats.shapiro(B)))            \n",
    "            write('    Wilcoxon_{} :: (Base-Stim) --> {}\\n'.format(len(A)-1, stats.wilcoxon(A, B)))\n",
    "        write('Kruskal ANOVA (Base) :: F={}\\n'.format(stats.kruskal(*np.array(A_arr))))\n",
    "        write('Kruskal ANOVA (Stim) :: F={}\\n'.format(stats.kruskal(*np.array(B_arr))))        \n",
    "        write('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Modes of Functional Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Cfg_Str_Mean':          {'label': 'Frequency Range'},\n",
    "            'Stim_Cfg_Sim':          {'label': 'Edge Correlation'}}\n",
    "\n",
    "# Generate a combined dataset\n",
    "df_AB = []\n",
    "for meas in analysis.keys():    \n",
    "    df = compute_prepost_coherence(meas)  \n",
    "    df = df[df['Stim_Type'] == 'stim']\n",
    "    df = df.groupby(['Subject_ID']).mean().reset_index()\n",
    "    df_AB.append(df)\n",
    "df_AB = pd.merge(*df_AB)\n",
    "        \n",
    "for delay_prefix in ['_Del_1', '_Del_2']:\n",
    "    # Seaborn params\n",
    "    sns_plot_params = {'x': analysis.keys()[0]+delay_prefix,\n",
    "                       'y': analysis.keys()[1]+delay_prefix,\n",
    "                       'data': df_AB}    \n",
    "    gg = sns.jointplot(stat_func=stats.spearmanr,\n",
    "                       **sns_plot_params)\n",
    "    gg.ax_marg_x.set_xlabel('')\n",
    "    gg.ax_marg_y.set_ylabel('')\n",
    "    \n",
    "    sns.set_context('paper')\n",
    "\n",
    "    #plt.savefig('{}/Delta-{}.BarPlot.svg'.format(path_Data['Figure'], 'example'))\n",
    "    plt.show()\n",
    "    #plt.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Stimulation Energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key = ['Subject_ID',\n",
    "                   'Stim_Anode',\n",
    "                   'Stim_Cathode',\n",
    "                   'Stim_Dur',\n",
    "                   'Stim_Freq',\n",
    "                   'Stim_Amp',\n",
    "                   'Coherence_ID']\n",
    "\n",
    "df_stim_sel = df_globaltopo_stim.groupby(unique_stim_key).indices\n",
    "\n",
    "# Use Keys that overlap between stim and base\n",
    "df_stim_key = df_stim_sel.keys()\n",
    "df_stim_key_set = np.array([set(k) for k in df_stim_key])\n",
    "            \n",
    "df_luts = {'stim': {'common_key': df_stim_key,\n",
    "                    'df_ind': df_stim_sel,\n",
    "                    'df_topo': df_globaltopo_stim}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Stimulation Energy Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Required import for following computations.\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from matplotlib.pyplot import figure, show\n",
    "\n",
    "\n",
    "def quad(plane='xy', origin=None, width=1, height=1, depth=0):\n",
    "    u, v = (0, 0) if origin is None else origin\n",
    "\n",
    "    plane = plane.lower()\n",
    "    if plane == 'xy':\n",
    "        vertices = ((u, v, depth),\n",
    "                    (u + width, v, depth),\n",
    "                    (u + width, v + height, depth),\n",
    "                    (u, v + height, depth))\n",
    "    elif plane == 'xz':\n",
    "        vertices = ((u, depth, v),\n",
    "                    (u + width, depth, v),\n",
    "                    (u + width, depth, v + height),\n",
    "                    (u, depth, v + height))\n",
    "    elif plane == 'yz':\n",
    "        vertices = ((depth, u, v),\n",
    "                    (depth, u + width, v),\n",
    "                    (depth, u + width, v + height),\n",
    "                    (depth, u, v + height))\n",
    "    else:\n",
    "        raise ValueError('\"{0}\" is not a supported plane!'.format(plane))\n",
    "\n",
    "    return np.array(vertices)\n",
    "\n",
    "\n",
    "def grid(plane='xy',\n",
    "         origin=None,\n",
    "         width=1,\n",
    "         height=1,\n",
    "         depth=0,\n",
    "         width_segments=1,\n",
    "         height_segments=1):\n",
    "    u, v = (0, 0) if origin is None else origin\n",
    "\n",
    "    w_x, h_y = width / width_segments, height / height_segments\n",
    "\n",
    "    quads = []\n",
    "    for i in range(width_segments):\n",
    "        for j in range(height_segments):\n",
    "            quads.append(\n",
    "                quad(plane, (i * w_x + u, j * h_y + v), w_x, h_y, depth))\n",
    "\n",
    "    return np.array(quads)\n",
    "\n",
    "\n",
    "def cube(plane=None,\n",
    "         origin=None,\n",
    "         width=1,\n",
    "         height=1,\n",
    "         depth=1,\n",
    "         width_segments=1,\n",
    "         height_segments=1,\n",
    "         depth_segments=1):\n",
    "    plane = (('+x', '-x', '+y', '-y', '+z', '-z')\n",
    "             if plane is None else\n",
    "             [p.lower() for p in plane])\n",
    "    u, v, w = (0, 0, 0) if origin is None else origin\n",
    "\n",
    "    w_s, h_s, d_s = width_segments, height_segments, depth_segments\n",
    "\n",
    "    grids = []\n",
    "    if '-z' in plane:\n",
    "        grids.extend(grid('xy', (u, w), width, depth, v, w_s, d_s))\n",
    "    if '+z' in plane:\n",
    "        grids.extend(grid('xy', (u, w), width, depth, v + height, w_s, d_s))\n",
    "\n",
    "    if '-y' in plane:\n",
    "        grids.extend(grid('xz', (u, v), width, height, w, w_s, h_s))\n",
    "    if '+y' in plane:\n",
    "        grids.extend(grid('xz', (u, v), width, height, w + depth, w_s, h_s))\n",
    "\n",
    "    if '-x' in plane:\n",
    "        grids.extend(grid('yz', (w, v), depth, height, u, d_s, h_s))\n",
    "    if '+x' in plane:\n",
    "        grids.extend(grid('yz', (w, v), depth, height, u + width, d_s, h_s))\n",
    "\n",
    "    return np.array(grids)\n",
    "\n",
    "\n",
    "stim_amp = np.sort(meta_ps['Stim_Amp'].unique())[1:] / 1000.\n",
    "stim_freq = np.sort(meta_ps['Stim_Freq'].unique())[1:]\n",
    "stim_dur = np.sort(meta_ps['Stim_Dur'].unique()) / 1000.\n",
    "\n",
    "n_amp = len(stim_amp)\n",
    "n_freq = len(stim_freq)\n",
    "n_dur = len(stim_dur)\n",
    "\n",
    "canvas = figure(figsize=(6,6), dpi=300.0)\n",
    "axes = Axes3D(canvas)\n",
    "quads = cube(width=stim_amp.max(), width_segments=n_amp,\n",
    "             height=stim_freq.max(), height_segments=n_freq,\n",
    "             depth=stim_dur.max(), depth_segments=n_dur)\n",
    "\n",
    "# define the colormap\n",
    "cmap = plt.get_cmap('magma')\n",
    "max_vals = stim_amp.max()*stim_freq.max()*stim_dur.max()\n",
    "vals = np.max(quads, axis=-2)\n",
    "vals = vals[:,0] * vals[:,1] * vals[:,2]\n",
    "\n",
    "\n",
    "# You can replace the following line by whatever suits you. Here, we compute\n",
    "# each quad colour by averaging its vertices positions.\n",
    "RGB = cmap(vals / max_vals)[:, :3]\n",
    "RGBA = np.hstack((RGB, np.full((RGB.shape[0], 1), .75)))\n",
    "\n",
    "collection = Poly3DCollection(quads, linewidths=0.0)\n",
    "collection.set_color(RGBA)\n",
    "axes.add_collection3d(collection)\n",
    "axes.set_xlim([0, stim_amp.max()])\n",
    "axes.set_ylim([0, stim_dur.max()])\n",
    "axes.set_zlim([0, stim_freq.max()])\n",
    "axes.view_init(azim=45)\n",
    "\n",
    "plt.savefig('{}/Stim_Energy.Cubed.svg'.format(path_Data['Figure'][reref]))\n",
    "\n",
    "show()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Post Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@memory.cache\n",
    "def compute_prepost_coherence_stimenergy(cfg_measure):\n",
    "    print('Processing: {}'.format(cfg_measure))\n",
    "        \n",
    "    # Initialize the measurement buckets (2 columns for post1/post2)\n",
    "    vals = {'Subject_ID': [],\n",
    "            'Coherence_ID': [],\n",
    "            'Stim_Type': [],\n",
    "            'Stim_Loc': [],\n",
    "            'Stim_Energy': [],\n",
    "            cfg_measure + '_Del_1': [],\n",
    "            cfg_measure + '_Del_2': []}\n",
    "    \n",
    "    # Condence the measurement across the common key criteria\n",
    "    for tp in df_luts.keys():\n",
    "        df_key = df_luts[tp]['common_key']\n",
    "        df_ind = df_luts[tp]['df_ind']        \n",
    "        df_topo = df_luts[tp]['df_topo']\n",
    "    \n",
    "        # Iterate over keys\n",
    "        for key_ii, key in enumerate(df_key):\n",
    "            df_sel = df_topo.iloc[df_ind[key]]\n",
    "\n",
    "            subj_id = df_sel['Subject_ID'].iloc[0]\n",
    "            coh_id = df_sel['Coherence_ID'].iloc[0]\n",
    "            stim_anode = df_sel['Stim_Anode'].iloc[0]\n",
    "            stim_cathode = df_sel['Stim_Cathode'].iloc[0]\n",
    "            stim_dur = df_sel['Stim_Dur'].iloc[0] / 1000.            \n",
    "            stim_freq = df_sel['Stim_Freq'].iloc[0]\n",
    "            stim_amp = df_sel['Stim_Amp'].iloc[0] / 1000.\n",
    "            stim_energy = stim_freq*stim_dur*stim_amp\n",
    "            \n",
    "            # Compute Measurement\n",
    "            if cfg_measure in ['Cfg_Str_Mean', 'Cfg_Str_Var']:\n",
    "                post1_vs_pre = (df_sel['Post1_' + cfg_measure] - df_sel['Pre_' + cfg_measure]).mean()\n",
    "                post2_vs_pre = (df_sel['Post2_' + cfg_measure] - df_sel['Pre_' + cfg_measure]).mean()                \n",
    "            elif cfg_measure in ['Stim_Cfg_Sim']:\n",
    "                post1_vs_pre = (df_sel[cfg_measure + '1']).mean()\n",
    "                post2_vs_pre = (df_sel[cfg_measure + '2']).mean()\n",
    "            elif cfg_measure in ['Stim_Node_Str_Delta']:\n",
    "                post1_vs_pre = np.mean([np.nanmean(ev < (0.05)/len(ev))\n",
    "                                        for ev in df_sel[cfg_measure + '1_pv']])\n",
    "                post2_vs_pre = np.mean([np.nanmean(ev < (0.05)/len(ev))\n",
    "                                        for ev in df_sel[cfg_measure + '2_pv']])\n",
    "            else:\n",
    "                raise ValueError('{} not a valid measurement'.format(cfg_measure))\n",
    "\n",
    "            # Add to the dictionary\n",
    "            vals['Subject_ID'].append(subj_id)\n",
    "            vals['Coherence_ID'].append(coh_id)\n",
    "            vals['Stim_Type'].append(tp)\n",
    "            vals['Stim_Loc'].append('_'.join(np.sort([stim_anode, stim_cathode])))\n",
    "            vals['Stim_Energy'].append(stim_energy)\n",
    "            vals[cfg_measure + '_Del_1'].append(post1_vs_pre)\n",
    "            vals[cfg_measure + '_Del_2'].append(post2_vs_pre)            \n",
    "    \n",
    "    # Convert to pandas dataframe\n",
    "    df = pd.DataFrame(vals, columns=vals.keys())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Comparative Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Cfg_Str_Mean':          {'xlabel': 'Frequency Range',\n",
    "                                      'ylabel': r'$\\rho(Stim Energy, Delta Mean Coherence)'},\n",
    "            'Cfg_Str_Var':           {'xlabel': 'Frequency Range',\n",
    "                                      'ylabel': r'$\\rho(Stim Energy, Delta Var Coherence)'},            \n",
    "            'Stim_Cfg_Sim':          {'xlabel': 'Frequency Range',\n",
    "                                      'ylabel': r'$\\rho(Stim Energy, Edge Correlation)'}}\n",
    "suffixes = ['_Del_1', '_Del_2']\n",
    "\n",
    "for meas in analysis.keys():\n",
    "    df = compute_prepost_coherence_stimenergy(meas)\n",
    "    suffixes = ['_Del_1', '_Del_2']\n",
    "    df = df.groupby(['Subject_ID', 'Stim_Energy', 'Coherence_ID']).mean().reset_index()\n",
    "    df_A = df.groupby(['Subject_ID', 'Coherence_ID'])[['Stim_Energy',\n",
    "                                                       meas+suffixes[0],\n",
    "                                                       meas+suffixes[1]]].corr('pearson').reset_index()\n",
    "    df_A = df_A[df_A['level_{}'.format(len(suffixes))] == 'Stim_Energy']\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        meas_full = meas + suffix   \n",
    "        \n",
    "        means = df_A.groupby('Coherence_ID').mean().reset_index()\n",
    "        \n",
    "        # Seaborn params\n",
    "        plt.figure()\n",
    "        ax = plt.subplot(111)        \n",
    "        sns_plot_params = {'x': 'Coherence_ID',\n",
    "                           'y': meas_full,\n",
    "                           'order': meta_dict['coherence_info'],\n",
    "                           'color': \".3\",\n",
    "                           'data': df_A,\n",
    "                           'ax': ax}\n",
    "        # Add a strip plot points            \n",
    "        ax = sns.stripplot(dodge=True,\n",
    "                           jitter=True,\n",
    "                           size=2,\n",
    "                           alpha=0.5,\n",
    "                           **sns_plot_params) \n",
    "        \n",
    "        sns_mean_params = sns_plot_params\n",
    "        sns_mean_params['x'] = 'Coherence_ID'\n",
    "        sns_mean_params['y'] = meas_full\n",
    "        sns_mean_params['color'] = sns.palettes.get_color_cycle()[0]\n",
    "        sns_mean_params['edgecolor'] = [0.0, 0.0, 0.0]\n",
    "        sns_mean_params['linewidth'] = 1.0\n",
    "        sns_mean_params['data'] = means\n",
    "        sns_mean_params['ax'] = ax\n",
    "        ax = sns.stripplot(dodge=False,\n",
    "                           jitter=False,\n",
    "                           size=12,\n",
    "                           **sns_mean_params)\n",
    "        \n",
    "        # Prettify the plotting + add labels\n",
    "        ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "        ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "        ax.set_ylim([-1.0, 1.0])\n",
    "        ax.set_title(meas_full)\n",
    "        sns.despine(ax=ax)\n",
    "        sns.set_context('paper')\n",
    "\n",
    "        #plt.savefig('{}/Delta-{}.BarPlot.svg'.format(path_Data['Figure'], 'example'))\n",
    "        plt.show()\n",
    "        #plt.close()\n",
    "                \n",
    "        # Stats\n",
    "        write(':: STATS ::\\n')\n",
    "        AA_arr = []\n",
    "        for coh_id in meta_dict['coherence_info']:\n",
    "            AA = df_A[df_A['Coherence_ID'] == coh_id][meas_full]\n",
    "            AA = AA[~np.isnan(AA)]\n",
    "            AA_arr.append(AA)  \n",
    "            \n",
    "            write('{}\\n'.format(coh_id))\n",
    "            write('    Shapiro        :: (Stim)      --> {}\\n'.format(stats.shapiro(AA)))\n",
    "            write('    T-test(1s)_{} :: (Stim)      --> {}\\n'.format(len(A)-1, stats.ttest_1samp(AA, 0)))\n",
    "        write('One-Way ANOVA (Stim)  :: F={}\\n'.format(stats.f_oneway(*np.array(AA_arr))))\n",
    "        write('\\n\\n\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Linear Regression for Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Cfg_Str_Mean':     {'xlabel': 'Func. Stim. Location (wgt. degree)',\n",
    "                                 'ylabel': 'Delta Mean Coherence',\n",
    "                                 'xlim': [0.0, 1.0],\n",
    "                                 'ylim': [-0.05, 0.05],\n",
    "                                 'stim': {'color': [0.2, 0.2, 0.2]}},\n",
    "            'Cfg_Str_Var':      {'xlabel': 'Func. Stim. Location (wgt. degree)',\n",
    "                                 'ylabel': 'Delta Var Coherence',\n",
    "                                 'xlim': [0.0, 1.0],\n",
    "                                 'ylim': [-0.005, 0.005],\n",
    "                                 'stim': {'color': [0.2, 0.2, 0.2]}},\n",
    "            'Cfg_Sim':          {'xlabel': 'Func. Stim. Location (wgt. degree)',\n",
    "                                 'ylabel': 'Edge Correlation (r-to-z)',\n",
    "                                 'xlim': [0.0, 1.0],\n",
    "                                 'ylim': [0.0, 2.0],                             \n",
    "                                 'stim': {'color': [0.2, 0.2, 0.2]}},\n",
    "            'Node_Str_Delta':   {'xlabel': 'Func. Stim. Location (wgt. degree)',\n",
    "                                 'ylabel': 'Fraction of Nodes Evoked',\n",
    "                                 'xlim': [0.0, 1.0],\n",
    "                                 'ylim': [0.0, 0.601],                             \n",
    "                                 'stim': {'color': [0.2, 0.2, 0.2]}}}            \n",
    "for meas in analysis.keys():\n",
    "    prepost_meas_coh, _, location, mni_coords = compute_prepost_coherence_stimparam(meas)\n",
    "    \n",
    "    plt.figure(figsize=(1.5,4), dpi=300.0)\n",
    "    for ii in xrange(len(meta_dict['coherence_info'])):\n",
    "        write('{}-{}\\n'.format(meas, meta_dict['coherence_info'][ii]))\n",
    "        \n",
    "        \n",
    "        ax = plt.subplot(4,1,ii+1)\n",
    "        ax.set_xlim(analysis[meas]['xlim'])\n",
    "        ax.set_ylim(analysis[meas]['ylim'])\n",
    "        ax.set_title(meta_dict['coherence_freq'][ii])\n",
    "        if ii == 3:\n",
    "            ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "        if ii == 1:\n",
    "            ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "        ax.locator_params(nbins=3, axis='both')\n",
    "        if (meas == 'Node_Str_Delta'):\n",
    "            ax.locator_params(nbins=3, axis='x')\n",
    "            ax.locator_params(nbins=4, axis='y')\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "            \n",
    "        for tp in ['stim']:\n",
    "            vals = prepost_meas_coh[tp]\n",
    "            all_location = location[tp][:, ii, 0]\n",
    "            unique_location = np.unique(all_location)\n",
    "            \n",
    "            mean = np.nan*np.zeros(len(unique_location))\n",
    "            serr = np.nan*np.zeros(len(unique_location))\n",
    "            for li, ll in enumerate(unique_location):\n",
    "                ix = np.flatnonzero(all_location == ll)\n",
    "                if meas == 'Cfg_Sim':\n",
    "                    delta = np.arctanh(vals[ix, ii, 0])\n",
    "                    delta = delta[~np.isinf(delta)]\n",
    "                elif meas == 'Node_Str_Delta': \n",
    "                    delta = vals[ix, ii, 0]\n",
    "                else:\n",
    "                    delta = vals[ix, ii, 1]-vals[ix, ii, 0]\n",
    "                mean[li] = np.nan_to_num(np.nanmean(delta))\n",
    "                serr[li] = np.nan_to_num(np.nanstd(delta)) / np.sqrt(len(ix))\n",
    "            \n",
    "            ax = sns.regplot(x=unique_location, y=mean,\n",
    "                             color=analysis[meas][tp]['color'],\n",
    "                             scatter_kws={'s':1.0}, ci=68, ax=ax)\n",
    "            \n",
    "            slp, yint, rv, pv, stdr = stats.linregress(unique_location,\n",
    "                                                       mean)\n",
    "            rv, pv = stats.spearmanr(unique_location, mean)\n",
    "            write('{}: n={:2.2f}, rv={:2.2f}, pv={:2.2e}\\n'.format(tp, len(unique_location), rv, pv))\n",
    "        write('\\n')\n",
    "    write('\\n\\n')\n",
    "\n",
    "    #plt.savefig('{}/{}-Func_Stim_Loc.LinReg.svg'.format(path_Data['Figure'][reref], meas))\n",
    "    plt.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Stimulation Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key = ['Subject_ID',\n",
    "                   'Stim_Anode',\n",
    "                   'Stim_Cathode',\n",
    "                   'Stim_Dur',\n",
    "                   'Coherence_ID']\n",
    "\n",
    "df_stim_sel = df_globaltopo_stim.groupby(unique_stim_key).indices\n",
    "df_base_sel = df_globaltopo_base.groupby(unique_stim_key).indices\n",
    "\n",
    "# Use Keys that overlap between stim and base\n",
    "df_base_key = df_base_sel.keys()\n",
    "df_stim_key = df_stim_sel.keys()\n",
    "df_base_key_set = np.array([set(k) for k in df_base_key])\n",
    "df_stim_key_set = np.array([set(k) for k in df_stim_key])\n",
    "\n",
    "df_base_common_key = []\n",
    "df_stim_common_key = []\n",
    "for b_k_ii, b_k in enumerate(df_base_key_set):\n",
    "    for s_k_ii, s_k in enumerate(df_stim_key_set):\n",
    "        if b_k == s_k:\n",
    "            df_base_common_key.append(df_base_key[b_k_ii])\n",
    "            df_stim_common_key.append(df_stim_key[s_k_ii])            \n",
    "            continue\n",
    "            \n",
    "df_luts = {'stim': {'common_key': df_stim_common_key,\n",
    "                    'df_ind': df_stim_sel,\n",
    "                    'df_topo': df_globaltopo_stim},\n",
    "           'base': {'common_key': df_base_common_key,\n",
    "                    'df_ind': df_base_sel,\n",
    "                    'df_topo': df_globaltopo_base}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Stimulation Locs (MNI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, _, node_str, node_coord = compute_prepost_coherence_stimparam('Node_Str_Delta')\n",
    "\n",
    "def plot_sensors(node_coord, node_vals):\n",
    "    from Echobase.Plotting import render_brain_connectivity\n",
    "\n",
    "    ### Collect surface data\n",
    "    # Vertices and triangles\n",
    "    verts_rh, trias_rh = nib.freesurfer.io.read_geometry('{}/fsaverage/surf/rh.pial'.format(path_AtlasData))\n",
    "    verts_lh, trias_lh = nib.freesurfer.io.read_geometry('{}/fsaverage/surf/lh.pial'.format(path_AtlasData))\n",
    "\n",
    "    n_rh_verts = verts_rh.shape[0]\n",
    "    n_lh_verts = verts_lh.shape[0]\n",
    "\n",
    "    verts = np.vstack((verts_rh, verts_lh))\n",
    "    trias = np.vstack((trias_rh, trias_lh+n_rh_verts))\n",
    "\n",
    "    label_scalars = 20*np.zeros(n_rh_verts+n_lh_verts)\n",
    "\n",
    "    view_angle = {'Cor_RL': [90.0, 90.0],\n",
    "                  'Sag_PA': [0.0, 90.0],\n",
    "                  'Sag_AP': [180.0, 90.0]}\n",
    "    \n",
    "    cmap = plt.get_cmap('coolwarm')\n",
    "    node_rgba = np.array([cmap(nn) for nn in node_vals])\n",
    "    node_size= np.array([4.0 for nn in node_coord])\n",
    "\n",
    "    render_brain_connectivity.mlab.close(all=True)    \n",
    "    engine = render_brain_connectivity.draw(verts, trias, label_scalars, 'binary', 10.0,\n",
    "                                            node_coords=node_coord, node_sizes=node_size, node_colors=node_rgba,\n",
    "                                            conn_list=None, conn_cmap=None)\n",
    "    pixmap = {}\n",
    "    for ang in view_angle.keys():\n",
    "        render_brain_connectivity.mlab.view(azimuth=view_angle[ang][0],\n",
    "                                            elevation=view_angle[ang][1])\n",
    "        pixmap['{}'.format(ang)] = render_brain_connectivity.mlab.screenshot(mode='rgba')\n",
    "    render_brain_connectivity.mlab.close(all=True)\n",
    "    \n",
    "    return pixmap\n",
    "\n",
    "\n",
    "unique_node_coord = []\n",
    "unique_node_str = []\n",
    "for ii in xrange(node_coord['stim'].shape[0]):\n",
    "    nc = tuple(node_coord['stim'][ii, :])\n",
    "    if np.linalg.norm(nc) > 150:\n",
    "        continue\n",
    "    if nc in unique_node_coord:\n",
    "        continue        \n",
    "        \n",
    "    unique_node_coord.append(nc)\n",
    "    unique_node_str.append(node_str['stim'][ii, :, :])\n",
    "node_coord = np.array(unique_node_coord)\n",
    "node_str = np.array(unique_node_str)\n",
    "\n",
    "\"\"\"\n",
    "sensor_pixmap = plot_sensors(node_coord, np.ones((node_coord.shape[0])))   \n",
    "plt.figure(figsize=(4.5,1.5), dpi=600.0)\n",
    "for jj, key in enumerate(['Sag_PA', 'Cor_RL', 'Sag_AP']):\n",
    "    ax = plt.subplot(1,3,jj+1)\n",
    "    ax.imshow(sensor_pixmap[key])\n",
    "    ax.set_axis_off()\n",
    "plt.savefig('{}/Stim_Location.freesurfer.svg'.format(path_Data['Figure'][reref]))\n",
    "plt.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Post Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@memory.cache\n",
    "def compute_prepost_coherence_stimloc(cfg_measure):\n",
    "    print('Processing: {}'.format(cfg_measure))\n",
    "        \n",
    "    # Initialize the measurement buckets (2 columns for post1/post2)\n",
    "    vals = {'Subject_ID': [],\n",
    "            'Coherence_ID': [],\n",
    "            'Stim_Type': [],\n",
    "            'Stim_Loc': [],\n",
    "            'Stim_NodeStr': [],\n",
    "            cfg_measure + '_Del_1': [],\n",
    "            cfg_measure + '_Del_2': []}\n",
    "    \n",
    "    # Condence the measurement across the common key criteria\n",
    "    for tp in ['stim']:\n",
    "        df_key = df_luts[tp]['common_key']\n",
    "        df_ind = df_luts[tp]['df_ind']        \n",
    "        df_topo = df_luts[tp]['df_topo']\n",
    "        \n",
    "        df_key_base = df_luts['base']['common_key']\n",
    "        df_ind_base = df_luts['base']['df_ind']        \n",
    "        df_topo_base = df_luts['base']['df_topo']\n",
    "        \n",
    "    \n",
    "        # Iterate over keys\n",
    "        for key_ii, key in enumerate(df_key):\n",
    "            df_sel = df_topo.iloc[df_ind[key]]\n",
    "            df_sel_base = df_topo_base.iloc[df_ind_base[df_key_base[key_ii]]]\n",
    "            \n",
    "            subj_id = df_sel['Subject_ID'].iloc[0]\n",
    "            coh_id = df_sel['Coherence_ID'].iloc[0]\n",
    "            \n",
    "            # Condense the labels for baseline (artifact_free)\n",
    "            stim_anode = df_sel_base['Stim_Anode'].iloc[0]\n",
    "            stim_cathode = df_sel_base['Stim_Cathode'].iloc[0]\n",
    "            stim_tag = '_'.join([stim_anode, stim_cathode])\n",
    "            \n",
    "            monop = meta_dict['electrode_loc'][subj_id]['Monopolar']\n",
    "            nonstim_lbl_artifact = np.unique(np.setdiff1d(monop['lbl_artifact'][stim_tag],\n",
    "                                                          np.array([stim_anode, stim_cathode])))\n",
    "            nonstim_ix_artifact = np.array([np.flatnonzero(monop['lbl'] == lbl)[0]\n",
    "                                            for lbl in nonstim_lbl_artifact])\n",
    "            goodchan_ix = np.setdiff1d(np.arange(len(monop['lbl'])),\n",
    "                                       nonstim_ix_artifact)\n",
    "            good_lbls = monop['lbl'][goodchan_ix]\n",
    "            \n",
    "            # Find the stim nodes in the electrode list and use base as reference node strengths\n",
    "            try:\n",
    "                anode_ix = np.flatnonzero(good_lbls == stim_anode)[0]\n",
    "                cathode_ix = np.flatnonzero(good_lbls == stim_cathode)[0]\n",
    "            except:\n",
    "                return good_lbls, df_key_base[key_ii], key\n",
    "            try:\n",
    "                print(subj_id)\n",
    "                stim_nodestr = []\n",
    "                for nodestr in df_sel_base['Pre_Node_Str']:\n",
    "                    stim_nodestr.append(0.5*(nodestr[anode_ix]+nodestr[cathode_ix]))\n",
    "                stim_nodestr = np.mean(stim_nodestr)\n",
    "            except:\n",
    "                print('---> ', subj_id)\n",
    "                continue\n",
    "            \n",
    "            # Compute Measurement\n",
    "            if cfg_measure in ['Cfg_Str_Mean', 'Cfg_Str_Var']:\n",
    "                post1_vs_pre = (df_sel['Post1_' + cfg_measure] - df_sel['Pre_' + cfg_measure]).mean()\n",
    "                post2_vs_pre = (df_sel['Post2_' + cfg_measure] - df_sel['Pre_' + cfg_measure]).mean()                \n",
    "            elif cfg_measure in ['Stim_Cfg_Sim']:\n",
    "                post1_vs_pre = (df_sel[cfg_measure + '1']).mean()\n",
    "                post2_vs_pre = (df_sel[cfg_measure + '2']).mean()\n",
    "            elif cfg_measure in ['Stim_Node_Str_Delta']:\n",
    "                post1_vs_pre = np.mean([np.nanmean(ev < (0.05)/len(ev))\n",
    "                                        for ev in df_sel[cfg_measure + '1_pv']])\n",
    "                post2_vs_pre = np.mean([np.nanmean(ev < (0.05)/len(ev))\n",
    "                                        for ev in df_sel[cfg_measure + '2_pv']])\n",
    "            else:\n",
    "                raise ValueError('{} not a valid measurement'.format(cfg_measure))\n",
    "\n",
    "            # Add to the dictionary\n",
    "            vals['Subject_ID'].append(subj_id)\n",
    "            vals['Coherence_ID'].append(coh_id)\n",
    "            vals['Stim_Type'].append(tp)\n",
    "            vals['Stim_Loc'].append('_'.join(np.sort([stim_anode, stim_cathode])))\n",
    "            vals['Stim_NodeStr'].append(stim_nodestr)\n",
    "            vals[cfg_measure + '_Del_1'].append(post1_vs_pre)\n",
    "            vals[cfg_measure + '_Del_2'].append(post2_vs_pre)            \n",
    "    \n",
    "    # Convert to pandas dataframe\n",
    "    df = pd.DataFrame(vals, columns=vals.keys())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Comparative Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = {'Cfg_Str_Mean':          {'xlabel': 'Frequency Range',\n",
    "                                      'ylabel': r'$\\rho(Stim NodeStr, Delta Mean Coherence)'},\n",
    "            'Cfg_Str_Var':           {'xlabel': 'Frequency Range',\n",
    "                                      'ylabel': r'$\\rho(Stim NodeStr, Delta Var Coherence)'},            \n",
    "            'Stim_Cfg_Sim':          {'xlabel': 'Frequency Range',\n",
    "                                      'ylabel': r'$\\rho(Stim NodeStr, Edge Correlation)'}}\n",
    "suffixes = ['_Del_1', '_Del_2']\n",
    "\n",
    "for meas in analysis.keys():\n",
    "    df = compute_prepost_coherence_stimloc(meas)\n",
    "    suffixes = ['_Del_1', '_Del_2']\n",
    "    df = df.groupby(['Subject_ID', 'Stim_NodeStr', 'Coherence_ID']).mean().reset_index()\n",
    "    df_A = df.groupby(['Subject_ID', 'Coherence_ID'])[['Stim_NodeStr',\n",
    "                                                       meas+suffixes[0],\n",
    "                                                       meas+suffixes[1]]].corr('pearson').reset_index()\n",
    "    df_A = df_A[df_A['level_{}'.format(len(suffixes))] == 'Stim_NodeStr']\n",
    "    \n",
    "    for suffix in suffixes:\n",
    "        meas_full = meas + suffix   \n",
    "        \n",
    "        means = df_A.groupby('Coherence_ID').mean().reset_index()\n",
    "        \n",
    "        # Seaborn params\n",
    "        plt.figure()\n",
    "        ax = plt.subplot(111)        \n",
    "        sns_plot_params = {'x': 'Coherence_ID',\n",
    "                           'y': meas_full,\n",
    "                           'order': meta_dict['coherence_info'],\n",
    "                           'color': \".3\",\n",
    "                           'data': df_A,\n",
    "                           'ax': ax}\n",
    "        # Add a strip plot points            \n",
    "        ax = sns.stripplot(dodge=True,\n",
    "                           jitter=True,\n",
    "                           size=2,\n",
    "                           alpha=0.5,\n",
    "                           **sns_plot_params) \n",
    "        \n",
    "        sns_mean_params = sns_plot_params\n",
    "        sns_mean_params['x'] = 'Coherence_ID'\n",
    "        sns_mean_params['y'] = meas_full\n",
    "        sns_mean_params['color'] = sns.palettes.get_color_cycle()[0]\n",
    "        sns_mean_params['edgecolor'] = [0.0, 0.0, 0.0]\n",
    "        sns_mean_params['linewidth'] = 1.0\n",
    "        sns_mean_params['data'] = means\n",
    "        sns_mean_params['ax'] = ax\n",
    "        ax = sns.stripplot(dodge=False,\n",
    "                           jitter=False,\n",
    "                           size=12,\n",
    "                           **sns_mean_params)\n",
    "        \n",
    "        # Prettify the plotting + add labels\n",
    "        ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "        ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "        ax.set_ylim([-1.0, 1.0])\n",
    "        ax.set_title(meas_full)\n",
    "        sns.despine(ax=ax)\n",
    "        sns.set_context('paper')\n",
    "\n",
    "        #plt.savefig('{}/Delta-{}.BarPlot.svg'.format(path_Data['Figure'], 'example'))\n",
    "        plt.show()\n",
    "        #plt.close()\n",
    "                \n",
    "        # Stats\n",
    "        write(':: STATS ::\\n')\n",
    "        AA_arr = []\n",
    "        for coh_id in meta_dict['coherence_info']:\n",
    "            AA = df_A[df_A['Coherence_ID'] == coh_id][meas_full]\n",
    "            AA = AA[~np.isnan(AA)]\n",
    "            AA_arr.append(AA)  \n",
    "            \n",
    "            write('{}\\n'.format(coh_id))\n",
    "            write('    Shapiro        :: (Stim)      --> {}\\n'.format(stats.shapiro(AA)))\n",
    "            write('    T-test(1s)_{} :: (Stim)      --> {}\\n'.format(len(A)-1, stats.ttest_1samp(AA, 0)))\n",
    "        write('One-Way ANOVA (Stim)  :: F={}\\n'.format(stats.f_oneway(*np.array(AA_arr))))\n",
    "        write('\\n\\n\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Stimulation to Structural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key =['Subject_ID',\n",
    "                  'Stim_Anode',\n",
    "                  'Stim_Cathode']\n",
    "meta_stim_type = meta_ps.loc[meta_ps.Stim_Type == 'stimulating']\n",
    "meta_sham_type = meta_ps.loc[meta_ps.Stim_Type == 'sham']\n",
    "\n",
    "meta_stim_lut = meta_stim_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_sham_lut = meta_sham_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_base_lut = meta_base.groupby(['Subject_ID'])['Base_ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Post Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def compute_structural_prepost_coherence_stimparam(cfg_measure):\n",
    "    print('Processing: {}'.format(cfg_measure))\n",
    "    \n",
    "    # Initialize the measurement buckets\n",
    "    if cfg_measure == 'Cfg_Sim':\n",
    "        vals = {'stim': np.nan*np.zeros((len(meta_stim_lut), len(meta_dict['coherence_info']), 1))}\n",
    "    else:     \n",
    "        vals = {'stim': np.nan*np.zeros((len(meta_stim_lut), len(meta_dict['coherence_info']), 2))}\n",
    "    location = {'stim': np.nan*np.zeros(len(meta_stim_lut))}    \n",
    "    luts = {'stim': meta_stim_lut}\n",
    "    \n",
    "    for tp in vals.keys():\n",
    "        tp_vals = vals[tp]\n",
    "        meta_lut = luts[tp]\n",
    "        stim_location = location[tp]        \n",
    "        \n",
    "        sel_globaltopo = df_globaltopo_stim\n",
    "    \n",
    "        # Iterate over coherence\n",
    "        for c_ii, coh_id in enumerate(meta_dict['coherence_info']):\n",
    "            sel_coh = sel_globaltopo[sel_globaltopo['Coherence_ID'] == coh_id]\n",
    "            sel_coh_base = df_globaltopo_base[df_globaltopo_base['Coherence_ID'] == coh_id]\n",
    "\n",
    "            # Iterate over unique stim parameters\n",
    "            for s_ii, (meta_key, ev_ids) in enumerate(meta_lut.iteritems()):\n",
    "                if tp in ['stim']:\n",
    "                    key_dict = dict(zip(unique_stim_key, meta_key))\n",
    "                    sel_key_subj = sel_coh.loc[sel_coh.Subject_ID == key_dict['Subject_ID']]\n",
    "                    sel_key_ev = sel_key_subj.loc[sel_key_subj.Event_ID.isin(ev_ids)]\n",
    "                    \n",
    "                    if cfg_measure == 'Cfg_Sim':\n",
    "                        tp_vals[s_ii, c_ii, 0] = np.nanmean(sel_key_ev['Stim_' + cfg_measure])\n",
    "                    elif cfg_measure == 'Node_Str_Delta':\n",
    "                        delta_pv = sel_key_ev['Stim_' + cfg_measure +'_pv']\n",
    "                        evoked_node_cnt = np.mean([np.mean(ev < (0.05/len(ev)))\n",
    "                                                   for ev in delta_pv])\n",
    "                        tp_vals[s_ii, c_ii, 0] = np.nan_to_num(evoked_node_cnt)\n",
    "                    else:\n",
    "                        tp_vals[s_ii, c_ii, 0] = np.nanmean(sel_key_ev['Pre_' + cfg_measure])\n",
    "                        tp_vals[s_ii, c_ii, 1] = np.nanmean(sel_key_ev['Post_' + cfg_measure])\n",
    "\n",
    "                # Get the channel indices involved in stimulation\n",
    "                jacksheet = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Jacksheet']\n",
    "                if reref == 'CommonAverage':\n",
    "                    stim_ix = []\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Anode']))\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Cathode']))    \n",
    "                    stim_ix = np.unique([iyy for ixx in stim_ix for iyy in ixx])            \n",
    "                else:\n",
    "                    raise Exception('reref currently supports CommonAverage')\n",
    "                    \n",
    "                \n",
    "                if not (key_dict['Subject_ID'] in meta_dict['struct_adj'][reref].keys()):\n",
    "                    continue\n",
    "                    \n",
    "                scale_id = 'scale500'\n",
    "                atlas_index = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Atlas_Index'][scale_id]\n",
    "                adj_struct = meta_dict['struct_adj'][reref][key_dict['Subject_ID']][scale_id]['GFA']['adj']\n",
    "                struct_degree = np.mean(adj_struct, axis=0)\n",
    "                el_degree = struct_degree[atlas_index]\n",
    "                stim_location[s_ii] = np.nan_to_num(np.nanmean(el_degree[stim_ix]))\n",
    "    \n",
    "        # Remove invalid observations\n",
    "        tp_vals = tp_vals[np.unique(np.nonzero(~np.isnan(tp_vals))[0]), :, :]\n",
    "        stim_location = stim_location[np.unique(np.nonzero(~np.isnan(tp_vals))[0])]\n",
    "        \n",
    "        tp_vals = tp_vals[~np.isnan(stim_location), :, :]\n",
    "        stim_location = stim_location[~np.isnan(stim_location)]\n",
    "        \n",
    "        vals[tp] = tp_vals\n",
    "        location[tp] = stim_location\n",
    "    return vals, location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Linear Regression for Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Cfg_Str_Mean':     {'xlabel': 'Struct. Stim. Location (wgt. degree)',\n",
    "                                 'ylabel': 'Delta Mean Coherence',\n",
    "                                 'xlim': [0.0, 0.01],\n",
    "                                 'ylim': [-0.05, 0.05],\n",
    "                                 'stim': {'color': [0.2, 0.2, 0.2]}},\n",
    "            'Cfg_Str_Var':      {'xlabel': 'Struct. Stim. Location (wgt. degree)',\n",
    "                                 'ylabel': 'Delta Var Coherence',\n",
    "                                 'xlim': [0.0, 0.005],\n",
    "                                 'ylim': [-0.01, 0.01],\n",
    "                                 'stim': {'color': [0.2, 0.2, 0.2]}},\n",
    "            'Cfg_Sim':          {'xlabel': 'Struct. Stim. Location (wgt. degree)',\n",
    "                                 'ylabel': 'Edge Correlation (r-to-z)',\n",
    "                                 'xlim': [0.0, 0.01],\n",
    "                                 'ylim': [0.0, 1.0],                             \n",
    "                                 'stim': {'color': [0.2, 0.2, 0.2]}},\n",
    "            'Node_Str_Delta':   {'xlabel': 'Struct. Stim. Location (wgt. degree)',\n",
    "                                 'ylabel': 'Fraction of Nodes Evoked',\n",
    "                                 'xlim': [0.0, 0.01],\n",
    "                                 'ylim': [0.0, 0.601],                             \n",
    "                                 'stim': {'color': [0.2, 0.2, 0.2]}}}            \n",
    "for meas in analysis.keys():\n",
    "    prepost_meas_coh, location = compute_structural_prepost_coherence_stimparam(meas)\n",
    "    \n",
    "    plt.figure(figsize=(1.5,4), dpi=300.0)\n",
    "    for ii in xrange(len(meta_dict['coherence_info'])):\n",
    "        write('{}-{}\\n'.format(meas, meta_dict['coherence_info'][ii]))\n",
    "        \n",
    "        ax = plt.subplot(4,1,ii+1)\n",
    "        ax.set_xlim(analysis[meas]['xlim'])\n",
    "        ax.set_ylim(analysis[meas]['ylim'])\n",
    "        ax.set_title(meta_dict['coherence_freq'][ii])\n",
    "        if ii == 3:\n",
    "            ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "        if ii == 1:\n",
    "            ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "        ax.locator_params(nbins=3, axis='both')\n",
    "        if meas == 'Node_Str_Delta':\n",
    "            ax.locator_params(nbins=3, axis='x')\n",
    "            ax.locator_params(nbins=4, axis='y')\n",
    "            \n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "            \n",
    "        for tp in ['stim']:\n",
    "            vals = prepost_meas_coh[tp]\n",
    "            all_location = location[tp]\n",
    "            unique_location = np.unique(all_location)\n",
    "            \n",
    "            mean = np.nan*np.zeros(len(unique_location))\n",
    "            serr = np.nan*np.zeros(len(unique_location))\n",
    "            for li, ll in enumerate(unique_location):\n",
    "                ix = np.flatnonzero(all_location == ll)\n",
    "                if meas == 'Cfg_Sim':\n",
    "                    delta = np.arctanh(vals[ix, ii, 0])\n",
    "                    delta = delta[~np.isnan(delta)]\n",
    "                elif meas == 'Node_Str_Delta':\n",
    "                    delta = vals[ix, ii, 0]\n",
    "                else:\n",
    "                    delta = vals[ix, ii, 1]-vals[ix, ii, 0]\n",
    "                mean[li] = np.nan_to_num(np.nanmean(delta))\n",
    "                serr[li] = np.nan_to_num(np.nanstd(delta)) / np.sqrt(len(ix))\n",
    "            \n",
    "            ax = sns.regplot(x=unique_location, y=mean,\n",
    "                             color=analysis[meas][tp]['color'],\n",
    "                             scatter_kws={'s':1.0}, ci=68, ax=ax)\n",
    "            \n",
    "            slp, yint, rv, pv, stdr = stats.linregress(unique_location, mean)\n",
    "            rv, pv = stats.spearmanr(unique_location, mean)\n",
    "            write('{}: rv={:2.2f}, pv={:2.2e}\\n'.format(tp, rv, pv))\n",
    "        write('\\n')\n",
    "    write('\\n\\n')\n",
    "\n",
    "    plt.savefig('{}/{}-Struct_Stim_Loc.GFA.LinReg.svg'.format(path_Data['Figure'][reref], meas))\n",
    "    plt.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Node Modulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Stimulation to Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key =['Subject_ID',\n",
    "                  'Stim_Anode',\n",
    "                  'Stim_Cathode']\n",
    "meta_stim_type = meta_ps.loc [meta_ps.Stim_Type == 'stimulating']\n",
    "meta_sham_type = meta_ps.loc[meta_ps.Stim_Type == 'sham']\n",
    "\n",
    "meta_stim_lut = meta_stim_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_sham_lut = meta_sham_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_base_lut = meta_base.groupby(['Subject_ID'])['Base_ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Coherence with Evoked Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def compute_baseline_coherence_evoked(cfg_measure):\n",
    "    print('Processing: {}'.format(cfg_measure))\n",
    "    \n",
    "    # Initialize the measurement buckets\n",
    "    vals = {'stim': np.nan*np.zeros((len(meta_stim_lut), len(meta_dict['coherence_info']), 2))}\n",
    "    luts = {'stim': meta_stim_lut}\n",
    "    \n",
    "    for tp in vals.keys():\n",
    "        tp_vals = vals[tp]\n",
    "        meta_lut = luts[tp]\n",
    "        \n",
    "        sel_globaltopo = df_globaltopo_stim\n",
    "    \n",
    "        # Iterate over coherence\n",
    "        for c_ii, coh_id in enumerate(meta_dict['coherence_info']):\n",
    "            sel_coh = sel_globaltopo[sel_globaltopo['Coherence_ID'] == coh_id]\n",
    "            sel_coh_base = df_globaltopo_base[df_globaltopo_base['Coherence_ID'] == coh_id]\n",
    "\n",
    "            # Iterate over unique stim parameters\n",
    "            for s_ii, (meta_key, ev_ids) in enumerate(meta_lut.iteritems()):\n",
    "                if tp in ['stim']:\n",
    "                    key_dict = dict(zip(unique_stim_key, meta_key))\n",
    "                    sel_key_subj = sel_coh.loc[sel_coh.Subject_ID == key_dict['Subject_ID']]\n",
    "                    sel_key_ev = sel_key_subj.loc[sel_key_subj.Event_ID.isin(ev_ids)]\n",
    "                    \n",
    "                    delta_pv = sel_key_ev['Stim_' + cfg_measure + '_pv']\n",
    "                    evoked_node_cnt = np.array([ev < (0.05/len(ev))\n",
    "                                                for ev in delta_pv])\n",
    "                    evoked_node_pct = np.mean(evoked_node_cnt, axis=0)\n",
    "                    \n",
    "                # Get the channel indices involved in stimulation\n",
    "                jacksheet = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Jacksheet']\n",
    "                if reref == 'CommonAverage':\n",
    "                    stim_ix = []\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Anode']))\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Cathode']))    \n",
    "                    stim_ix = np.unique([iyy for ixx in stim_ix for iyy in ixx])            \n",
    "                else:\n",
    "                    raise Exception('reref currently supports CommonAverage')\n",
    "                \n",
    "                sel_base_subj = sel_coh_base.loc[sel_coh_base.Subject_ID == key_dict['Subject_ID']]\n",
    "                adj = sel_base_subj['Base_Adj_Matr_Mean'].iloc[0]\n",
    "                stim_coh = np.delete(np.nanmean(adj[stim_ix, :], axis=0), stim_ix)\n",
    "                                \n",
    "                try:                    \n",
    "                    rv, pv = stats.pearsonr(stim_coh, evoked_node_pct)\n",
    "                    tp_vals[s_ii, c_ii, 0] = np.arctanh(rv)\n",
    "\n",
    "                    rvs = []\n",
    "                    for perm_ii in xrange(1000):\n",
    "                        stim_null_ix = np.random.permutation(len(evoked_node_pct))[:len(stim_ix)]\n",
    "                        stim_coh_null = np.delete(np.nanmean(adj[stim_null_ix, :], axis=0), stim_null_ix)\n",
    "                        rv, pv = stats.pearsonr(stim_coh_null, evoked_node_pct)\n",
    "                        rvs.append(np.arctanh(np.nan_to_num(rv)))\n",
    "                    tp_vals[s_ii, c_ii, 1] = np.nan_to_num(np.mean(rvs))\n",
    "                except Exception as E:\n",
    "                    tp_vals[s_ii, c_ii, 0] = np.nan\n",
    "                    tp_vals[s_ii, c_ii, 1] = np.nan                    \n",
    "                \n",
    "        # Remove invalid observations\n",
    "        tp_vals = tp_vals[np.unique(np.nonzero(~np.isnan(tp_vals))[0]), :, :]\n",
    "        \n",
    "        vals[tp] = tp_vals\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Comparative Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Node_Str_Delta':   {'xlabel': 'Frequency Range',\n",
    "                                 'ylabel': 'Coherence with Evoked Nodes (r-to-z)',\n",
    "                                 'xlim': [-0.5, len(meta_dict['coherence_info'])-0.5],\n",
    "                                 'ylim': [0.0, 0.4],\n",
    "                                 'stim': {'color': [1.0, 0.3, 0.3]},\n",
    "                                 'null': {'color': [0.2, 0.2, 0.2]}}}\n",
    "n_grp = len(meta_dict['coherence_freq'])            \n",
    "\n",
    "for meas in analysis.keys():\n",
    "    baseline_coh_corr = compute_baseline_coherence_evoked(meas)\n",
    "        \n",
    "    all_corr = []\n",
    "    all_mean = []\n",
    "    all_serr = []\n",
    "    all_color = []\n",
    "    all_pos = []\n",
    "    for tp_ii, tp in enumerate(['stim', 'null']):\n",
    "        corr = baseline_coh_corr['stim'][:, :, tp_ii]\n",
    "        all_corr.append(corr)\n",
    "        for nn in xrange(corr.shape[1]):\n",
    "            all_mean.append(np.nanmean(corr[:, nn]))\n",
    "            all_serr.append(np.nanstd(corr[:, nn]) / np.sqrt(corr.shape[0]))\n",
    "            all_color.append(analysis[meas][tp]['color'])\n",
    "            all_pos.append(nn + 0.2*tp_ii)\n",
    "        \n",
    "    plt.figure(figsize=(2,1.5), dpi=300.0)\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    ax.bar(left=all_pos, height=all_mean, yerr=all_serr,\n",
    "           width=0.2, color=all_color, lw=0.0)\n",
    "    \n",
    "    ax.set_xlim(analysis[meas]['xlim'])\n",
    "    ax.set_ylim(analysis[meas]['ylim'])\n",
    "    ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "    ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "    ax.xaxis.set_ticks(np.arange(n_grp)+0.2)\n",
    "    ax.xaxis.set_ticklabels(meta_dict['coherence_freq'])\n",
    "    ax.locator_params(nbins=4, axis='y')    \n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    plt.savefig('{}/Evoked_Nodes-Func_Coherence.BarPlot.svg'.format(path_Data['Figure'][reref]))\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    for ff in xrange(len(meta_dict['coherence_freq'])):\n",
    "        write('{}\\n'.format(meta_dict['coherence_freq'][ff]))\n",
    "        write('t-test: {}\\n'.format(stats.ttest_rel(all_corr[0][:, ff], all_corr[1][:, ff])))\n",
    "        write('cohens: {}\\n\\n'.format(cohens_d(all_corr[0][:, ff], all_corr[1][:, ff])))\n",
    "        write('df: {}\\n'.format(len(all_corr[0][:, ff])))\n",
    "    write('Anova: {}\\n'.format(stats.f_oneway(*all_corr[0].T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Stimulation to Structural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key =['Subject_ID',\n",
    "                  'Stim_Anode',\n",
    "                  'Stim_Cathode']\n",
    "meta_stim_type = meta_ps.loc[meta_ps.Stim_Type == 'stimulating']\n",
    "meta_sham_type = meta_ps.loc[meta_ps.Stim_Type == 'sham']\n",
    "\n",
    "meta_stim_lut = meta_stim_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_sham_lut = meta_sham_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_base_lut = meta_base.groupby(['Subject_ID'])['Base_ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural Connectivity with Evoked Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def compute_structural_evoked(cfg_measure):\n",
    "    print('Processing: {}'.format(cfg_measure))\n",
    "    \n",
    "    # Initialize the measurement buckets\n",
    "    vals = {'stim': np.nan*np.zeros((len(meta_stim_lut), len(meta_dict['coherence_info']), 2))}\n",
    "    luts = {'stim': meta_stim_lut}\n",
    "    \n",
    "    for tp in vals.keys():\n",
    "        tp_vals = vals[tp]\n",
    "        meta_lut = luts[tp]\n",
    "        \n",
    "        sel_globaltopo = df_globaltopo_stim\n",
    "    \n",
    "        # Iterate over coherence\n",
    "        for c_ii, coh_id in enumerate(meta_dict['coherence_info']):\n",
    "            sel_coh = sel_globaltopo[sel_globaltopo['Coherence_ID'] == coh_id]\n",
    "            sel_coh_base = df_globaltopo_base[df_globaltopo_base['Coherence_ID'] == coh_id]\n",
    "\n",
    "            # Iterate over unique stim parameters\n",
    "            for s_ii, (meta_key, ev_ids) in enumerate(meta_lut.iteritems()):\n",
    "                if tp in ['stim']:\n",
    "                    key_dict = dict(zip(unique_stim_key, meta_key))\n",
    "                    sel_key_subj = sel_coh.loc[sel_coh.Subject_ID == key_dict['Subject_ID']]\n",
    "                    sel_key_ev = sel_key_subj.loc[sel_key_subj.Event_ID.isin(ev_ids)]\n",
    "                    \n",
    "                    delta_pv = sel_key_ev['Stim_' + cfg_measure + '_pv']\n",
    "                    evoked_node_cnt = np.array([ev < (0.05/len(ev))\n",
    "                                                for ev in delta_pv])\n",
    "                    evoked_node_pct = np.mean(evoked_node_cnt, axis=0)\n",
    "                    \n",
    "                # Get the channel indices involved in stimulation\n",
    "                jacksheet = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Jacksheet']\n",
    "                if reref == 'CommonAverage':\n",
    "                    stim_ix = []\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Anode']))\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Cathode']))    \n",
    "                    stim_ix = np.unique([iyy for ixx in stim_ix for iyy in ixx])            \n",
    "                else:\n",
    "                    raise Exception('reref currently supports CommonAverage')\n",
    "                nsstim_ix = np.setdiff1d(np.arange(len(jacksheet)), stim_ix)\n",
    "                    \n",
    "                if not (key_dict['Subject_ID'] in meta_dict['struct_adj'][reref].keys()):\n",
    "                    continue\n",
    "                    \n",
    "                scale_id = 'scale60'\n",
    "                atlas_index = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Atlas_Index'][scale_id]\n",
    "                adj_struct = meta_dict['struct_adj'][reref][key_dict['Subject_ID']][scale_id]['QA']['adj']\n",
    "                adj = adj_struct[atlas_index, :][:, atlas_index]\n",
    "                             \n",
    "                stim_coh = np.delete(np.nanmean(adj[stim_ix, :], axis=0), stim_ix)\n",
    "                rv, pv = stats.pearsonr(stim_coh, evoked_node_pct)\n",
    "                tp_vals[s_ii, c_ii, 0] = np.arctanh(np.nan_to_num(rv))\n",
    "\n",
    "                rvs = []\n",
    "                for perm_ii in xrange(1000):\n",
    "                    null_stim_ix = np.random.permutation(nsstim_ix)[:len(stim_ix)]\n",
    "                    stim_coh_null = np.delete(np.nanmean(adj[null_stim_ix, :], axis=0), stim_ix)\n",
    "                    rv, pv = stats.pearsonr(stim_coh_null, evoked_node_pct)\n",
    "                    rvs.append(np.arctanh(np.nan_to_num(rv)))\n",
    "                tp_vals[s_ii, c_ii, 1] = np.nan_to_num(np.nanmean(rvs))\n",
    "                \n",
    "        # Remove invalid observations\n",
    "        tp_vals = tp_vals[np.unique(np.nonzero(~np.isnan(tp_vals))[0]), :, :]\n",
    "        \n",
    "        vals[tp] = tp_vals\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Comparative Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Node_Str_Delta':   {'xlabel': 'Frequency Range',\n",
    "                                 'ylabel': 'Connectivity with Evoked Nodes (r-to-z)',\n",
    "                                 'xlim': [-0.5, len(meta_dict['coherence_info'])-0.5],\n",
    "                                 'ylim': [-0.1, 0.1],\n",
    "                                 'stim': {'color': [1.0, 0.3, 0.3]},\n",
    "                                 'null': {'color': [0.2, 0.2, 0.2]}}}\n",
    "n_grp = len(meta_dict['coherence_freq'])            \n",
    "\n",
    "for meas in analysis.keys():\n",
    "    baseline_coh_corr = compute_structural_evoked(meas)\n",
    "        \n",
    "    all_corr = []\n",
    "    all_mean = []\n",
    "    all_serr = []\n",
    "    all_color = []\n",
    "    all_pos = []\n",
    "    for tp_ii, tp in enumerate(['stim', 'null']):\n",
    "        corr = baseline_coh_corr['stim'][:, :, tp_ii]\n",
    "        all_corr.append(corr)\n",
    "        for nn in xrange(corr.shape[1]):\n",
    "            all_mean.append(np.nanmean(corr[:, nn]))\n",
    "            all_serr.append(np.nanstd(corr[:, nn]) / np.sqrt(corr.shape[0]))\n",
    "            all_color.append(analysis[meas][tp]['color'])\n",
    "            all_pos.append(nn + 0.2*tp_ii)\n",
    "        \n",
    "    plt.figure(figsize=(2,1.5), dpi=300.0)\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    ax.bar(left=all_pos, height=all_mean, yerr=all_serr,\n",
    "           width=0.2, color=all_color, lw=0.0)\n",
    "    \n",
    "    ax.set_xlim(analysis[meas]['xlim'])\n",
    "    ax.set_ylim(analysis[meas]['ylim'])\n",
    "    ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "    ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "    ax.xaxis.set_ticks(np.arange(n_grp)+0.2)\n",
    "    ax.xaxis.set_ticklabels(meta_dict['coherence_freq'])   \n",
    "    ax.locator_params(nbins=4, axis='y')    \n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    plt.savefig('{}/Evoked_Nodes-Struct_Connectivity.GFA.BarPlot.svg'.format(path_Data['Figure'][reref]))\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    for ff in xrange(len(meta_dict['coherence_freq'])):\n",
    "        write('{}\\n'.format(meta_dict['coherence_freq'][ff]))\n",
    "        write('t-test: {}\\n'.format(stats.ttest_rel(all_corr[0][:, ff], all_corr[1][:, ff])))\n",
    "        write('cohens: {}\\n\\n'.format(cohens_d(all_corr[0][:, ff], all_corr[1][:, ff])))\n",
    "    write('Anova: {}\\n'.format(stats.f_oneway(*all_corr[0].T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relating Structural Control and Structural Topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key =['Subject_ID',\n",
    "                  'Stim_Anode',\n",
    "                  'Stim_Cathode']\n",
    "meta_stim_type = meta_ps.loc[meta_ps.Stim_Type == 'stimulating']\n",
    "meta_sham_type = meta_ps.loc[meta_ps.Stim_Type == 'sham']\n",
    "\n",
    "meta_stim_lut = meta_stim_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_sham_lut = meta_sham_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_base_lut = meta_base.groupby(['Subject_ID'])['Base_ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controllability and connectivity of stim location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def compute_structural_control():\n",
    "    print('Processing: {}'.format('Structural Control'))\n",
    "    \n",
    "    # Initialize the measurement buckets\n",
    "    vals = {'stim': np.nan*np.zeros((len(meta_stim_lut), len(meta_dict['coherence_info']), 4))}\n",
    "    luts = {'stim': meta_stim_lut}\n",
    "    \n",
    "    for tp in vals.keys():\n",
    "        tp_vals = vals[tp]\n",
    "        meta_lut = luts[tp]\n",
    "\n",
    "        # Iterate over coherence\n",
    "        for c_ii, coh_id in enumerate(meta_dict['coherence_info']):\n",
    "            sel_coh_base = df_globaltopo_base[df_globaltopo_base['Coherence_ID'] == coh_id]            \n",
    "            \n",
    "            # Iterate over unique stim parameters\n",
    "            for s_ii, (meta_key, ev_ids) in enumerate(meta_lut.iteritems()):\n",
    "                key_dict = dict(zip(unique_stim_key, meta_key))\n",
    "\n",
    "                # Get the channel indices involved in stimulation\n",
    "                jacksheet = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Jacksheet']\n",
    "                if reref == 'CommonAverage':\n",
    "                    stim_ix = []\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Anode']))\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Cathode']))    \n",
    "                    stim_ix = np.unique([iyy for ixx in stim_ix for iyy in ixx])            \n",
    "                else:\n",
    "                    raise Exception('reref currently supports CommonAverage')\n",
    "                nsstim_ix = np.setdiff1d(np.arange(len(jacksheet)), stim_ix)\n",
    "                \n",
    "                sel_base_subj = sel_coh_base.loc[sel_coh_base.Subject_ID == key_dict['Subject_ID']]                \n",
    "                adj = sel_base_subj['Base_Adj_Matr_Mean'].iloc[0]                \n",
    "                \n",
    "                func_degr = np.nanmean(adj[:, stim_ix])                \n",
    "\n",
    "                if (key_dict['Subject_ID'] in meta_dict['struct_adj'][reref].keys()):\n",
    "                    scale_id = 'scale250'\n",
    "                    atlas_index = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Atlas_Index'][scale_id]\n",
    "                    adj_struct = meta_dict['struct_adj'][reref][key_dict['Subject_ID']][scale_id]['QA']['adj']\n",
    "\n",
    "                    struct_degr = np.mean(adj_struct, axis=0) \n",
    "                    struct_average = structural_control.average_control(adj_struct)\n",
    "                    struct_modal = structural_control.modal_control(adj_struct)            \n",
    "\n",
    "                    tp_vals[s_ii, c_ii, 0] = func_degr\n",
    "                    tp_vals[s_ii, c_ii, 1] = np.nanmean(struct_degr[atlas_index][stim_ix])\n",
    "                    tp_vals[s_ii, c_ii, 2] = np.nanmean(struct_average[atlas_index][stim_ix])\n",
    "                    tp_vals[s_ii, c_ii, 3] = np.nanmean(struct_modal[atlas_index][stim_ix])\n",
    "\n",
    "        # Remove invalid observations\n",
    "        tp_vals = tp_vals[np.unique(np.nonzero(~np.isnan(tp_vals))[0]), :]\n",
    "        \n",
    "        vals[tp] = tp_vals\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Baseline':     {'xlabel': 'Frequency Range',\n",
    "                             'ylabel': 'Corr(Modal, Baseline)',\n",
    "                             'xlim': [-0.5, len(meta_dict['coherence_info'])-0.5],\n",
    "                             'ylim': [0.0, 0.601],\n",
    "                             'color': [0.2, 0.2, 0.2]},  \n",
    "            'Average_Cntl': {'xlabel': 'Stim. Location (wgt. degree)',\n",
    "                             'ylabel': 'Controllability',\n",
    "                             'xlim': [0.0, 0.01],\n",
    "                             'ylim': [1.0, 1.1],\n",
    "                             'color': [0.2, 0.2, 0.2]},\n",
    "            'Modal_Cntl':   {'xlabel': 'Stim. Location (wgt. degree)',\n",
    "                             'ylabel': 'Controllability',\n",
    "                             'xlim': [0.0, 0.01],\n",
    "                             'ylim': [0.96, 1.00],                             \n",
    "                             'color': [0.2, 0.2, 0.2]}}\n",
    "            \n",
    "structural_meas_coh = compute_structural_control()\n",
    "\n",
    "# Plot Structure to Baseline\n",
    "meas = 'Baseline'\n",
    "plt.figure(figsize=(1.5,4), dpi=300.0)\n",
    "for ii in xrange(len(meta_dict['coherence_info'])):\n",
    "    write('{}-{}\\n'.format(meas, meta_dict['coherence_info'][ii]))\n",
    "\n",
    "    ax = plt.subplot(4,1,ii+1)    \n",
    "    ax.set_xlim(analysis[meas]['xlim'])\n",
    "    ax.set_ylim(analysis[meas]['ylim'])\n",
    "    if ii == 3:\n",
    "        ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "    if ii == 1:\n",
    "        ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "    ax.locator_params(nbins=3, axis='both')\n",
    "    ax.set_title(meta_dict['coherence_freq'][ii])\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    ax = sns.regplot(x=structural_meas_coh['stim'][:, ii, 1],\n",
    "                     y=structural_meas_coh['stim'][:, ii, 0],\n",
    "                     color=analysis[meas]['color'],\n",
    "                     scatter_kws={'s':1.0}, ci=68, ax=ax)\n",
    "\n",
    "    slp, yint, rv, pv, stdr = stats.linregress(structural_meas_coh['stim'][:, ii, 1],\n",
    "                                               structural_meas_coh['stim'][:, ii, 0])\n",
    "    rv, pv = stats.spearmanr(structural_meas_coh['stim'][:, ii, 1],\n",
    "                             structural_meas_coh['stim'][:, ii, 0])\n",
    "    write('rv={:2.2f}, pv={:2.2e}\\n'.format(rv, pv))\n",
    "write('\\n\\n')\n",
    "plt.savefig('{}/Struct_Stim_Loc-Func_Stim_Loc.QA.LinReg.svg'.format(path_Data['Figure'][reref]))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plot Structure to Baseline\n",
    "meas = 'Baseline'\n",
    "all_pos = []\n",
    "all_corr = []\n",
    "all_color = []\n",
    "for ii in xrange(len(meta_dict['coherence_info'])):\n",
    "    write('{}-{}\\n'.format(meas, meta_dict['coherence_info'][ii]))\n",
    "    \n",
    "    rv, pv = stats.spearmanr(structural_meas_coh['stim'][:, ii, 0],\n",
    "                             structural_meas_coh['stim'][:, 0, 3])            \n",
    "\n",
    "    all_pos.append(ii)    \n",
    "    all_corr.append(rv)\n",
    "    if pv < (0.05 / 4):\n",
    "        all_color.append([1.0, 0.0, 0.0])\n",
    "    else:\n",
    "        all_color.append(analysis[meas]['color'])\n",
    "    write('rv={:2.2f}, pv={:2.2e}\\n'.format(rv, pv))\n",
    "write('\\n\\n')\n",
    "\n",
    "plt.figure(figsize=(2,1.5), dpi=300.0)\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(left=all_pos, height=all_corr,\n",
    "       width=0.3, color=all_color, lw=0.0)    \n",
    "ax.set_xlim(analysis[meas]['xlim'])\n",
    "ax.set_ylim(analysis[meas]['ylim'])\n",
    "ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "ax.xaxis.set_ticks(np.arange(len(meta_dict['coherence_info']))+0.15)\n",
    "ax.xaxis.set_ticklabels(meta_dict['coherence_freq'])   \n",
    "ax.locator_params(nbins=4, axis='y')    \n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "plt.savefig('{}/Funct_Stim_Loc-Controllability.QA.LinReg.svg'.format(path_Data['Figure'][reref]))\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plot Controllability to Structure\n",
    "plt.figure(figsize=(1.5,4), dpi=300.0)\n",
    "for m_ii, meas in enumerate(['Average_Cntl', 'Modal_Cntl']):\n",
    "    write('{}\\n'.format(meas))\n",
    "    \n",
    "    ax = plt.subplot(4,1,m_ii+1)    \n",
    "    ax.set_xlim(analysis[meas]['xlim'])\n",
    "    ax.set_ylim(analysis[meas]['ylim'])\n",
    "    if m_ii == 1:\n",
    "        ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "    if m_ii == 0:\n",
    "        ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "    ax.set_title(meas)\n",
    "    ax.locator_params(nbins=3, axis='both')        \n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    ax = sns.regplot(x=structural_meas_coh['stim'][:, 0, 1],\n",
    "                     y=structural_meas_coh['stim'][:, 0, m_ii+2],\n",
    "                     color=analysis[meas]['color'],\n",
    "                     scatter_kws={'s':1.0}, ci=68, ax=ax)\n",
    "\n",
    "    slp, yint, rv, pv, stdr = stats.linregress(structural_meas_coh['stim'][:, 0, 1],\n",
    "                                               structural_meas_coh['stim'][:, 0, m_ii+2])\n",
    "    rv, pv = stats.spearmanr(structural_meas_coh['stim'][:, 0, 1],\n",
    "                             structural_meas_coh['stim'][:, 0, m_ii+2])    \n",
    "    write('rv={:2.2f}, pv={:2.2e}\\n'.format(rv, pv))\n",
    "write('\\n\\n')\n",
    "plt.savefig('{}/Struct_Stim_Loc-Controllability.QA.LinReg.svg'.format(path_Data['Figure'][reref]))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modulation Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key =['Subject_ID',\n",
    "                  'Stim_Anode',\n",
    "                  'Stim_Cathode']\n",
    "meta_stim_type = meta_ps.loc[meta_ps.Stim_Type == 'stimulating']\n",
    "meta_sham_type = meta_ps.loc[meta_ps.Stim_Type == 'sham']\n",
    "\n",
    "meta_stim_lut = meta_stim_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_sham_lut = meta_sham_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_base_lut = meta_base.groupby(['Subject_ID'])['Base_ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def compute_evoked_map(cfg_measure):\n",
    "    print('Processing: {}'.format(cfg_measure))\n",
    "    \n",
    "    # Initialize the measurement buckets\n",
    "    evoke_map = {}\n",
    "    for scale_id in meta_dict['atlas_info'][reref].keys():\n",
    "        evoke_map[scale_id] = {}\n",
    "        \n",
    "        atlas_label = meta_dict['atlas_info'][reref][scale_id].keys()\n",
    "    \n",
    "        # Iterate over coherence\n",
    "        for c_ii, coh_id in enumerate(meta_dict['coherence_info']):\n",
    "            evoke_map[scale_id][coh_id] = {'stim': np.zeros((len(atlas_label), len(atlas_label))),\n",
    "                                           'count': np.zeros((len(atlas_label), len(atlas_label)))}\n",
    "            \n",
    "            sel_coh = df_globaltopo_stim[df_globaltopo_stim['Coherence_ID'] == coh_id]\n",
    "\n",
    "            # Iterate over unique stim parameters\n",
    "            for s_ii, (meta_key, ev_ids) in enumerate(meta_stim_lut.iteritems()):\n",
    "                key_dict = dict(zip(unique_stim_key, meta_key))\n",
    "                sel_key_subj = sel_coh.loc[sel_coh.Subject_ID == key_dict['Subject_ID']]\n",
    "                sel_key_ev = sel_key_subj.loc[sel_key_subj.Event_ID.isin(ev_ids)]\n",
    "\n",
    "                delta_pv = sel_key_ev['Stim_' + cfg_measure + '_pv']\n",
    "                evoked_pv = np.array([(ev < 0.05/len(ev)) for ev in delta_pv])\n",
    "                if len(evoked_pv) == 0:\n",
    "                    continue\n",
    "                evoked_pv_mean = np.mean(evoked_pv, axis=0)\n",
    "\n",
    "                # Get the channel indices involved in stimulation\n",
    "                jacksheet = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Jacksheet']\n",
    "                if reref == 'CommonAverage':\n",
    "                    stim_ix = []\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Anode']))\n",
    "                    stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Cathode']))    \n",
    "                    stim_ix = np.unique([iyy for ixx in stim_ix for iyy in ixx])            \n",
    "                else:\n",
    "                    raise Exception('reref currently supports CommonAverage')\n",
    "                    meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]\n",
    "                nonstim_ix = np.setdiff1d(np.arange(len(jacksheet)), stim_ix)\n",
    "\n",
    "                eloc_atlas_idx = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Atlas_Index'][scale_id]\n",
    "                stim_atlas_idx = eloc_atlas_idx[stim_ix]\n",
    "                nonstim_atlas_idx = eloc_atlas_idx[nonstim_ix]\n",
    "\n",
    "                for s_idx in stim_atlas_idx:\n",
    "                    for ns_ii, ns_idx in enumerate(nonstim_atlas_idx):\n",
    "                        evoke_map[scale_id][coh_id]['stim'][s_idx, ns_idx] = evoked_pv_mean[ns_ii]\n",
    "                        evoke_map[scale_id][coh_id]['count'][s_idx, ns_idx] += 1.0\n",
    "    return evoke_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emap = compute_evoked_map('Node_Str_Delta')\n",
    "\n",
    "plt.matshow(emap['scale500']['AlphaTheta']['stim'] / \\\n",
    "            emap['scale500']['AlphaTheta']['count'], cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key =['Subject_ID',\n",
    "                  'Stim_Anode',\n",
    "                  'Stim_Cathode']\n",
    "meta_stim_type = meta_ps.loc[meta_ps.Stim_Type == 'stimulating']\n",
    "meta_sham_type = meta_ps.loc[meta_ps.Stim_Type == 'sham']\n",
    "\n",
    "meta_stim_lut = meta_stim_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_sham_lut = meta_sham_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_base_lut = meta_base.groupby(['Subject_ID'])['Base_ID'].unique()\n",
    "\n",
    "beh_dict = meta_dict['behavior_info'][reref]\n",
    "#beh_dict = beh_dict[beh_dict.Experiment_ID.isin(['FR2/catFR2', 'FR3/catFR3', 'FR5/catFR5'])]\n",
    "#beh_dict = beh_dict[beh_dict.Experiment_ID.isin(['PAL2', 'PAL3', 'PAL5'])]                  \n",
    "meta_behv_lut = beh_dict.groupby(unique_stim_key)['Delta_Behavior'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@memory.cache\n",
    "def compute_baseline_behavior():\n",
    "    \n",
    "    # Initialize the measurement buckets\n",
    "    vals = np.nan*np.zeros((len(meta_behv_lut), len(meta_dict['coherence_info']), 2))\n",
    "    null_vals = np.nan*np.zeros((len(meta_behv_lut), len(meta_dict['coherence_info']), 10000))\n",
    "    \n",
    "    # Iterate over coherence\n",
    "    for c_ii, coh_id in enumerate(meta_dict['coherence_info']):\n",
    "        sel_coh_base = df_globaltopo_base[df_globaltopo_base['Coherence_ID'] == coh_id]\n",
    "        \n",
    "        # Iterate over unique stim parameters\n",
    "        for s_ii, (meta_key, del_behs) in enumerate(meta_behv_lut.iteritems()):\n",
    "            key_dict = dict(zip(unique_stim_key, meta_key))\n",
    "              \n",
    "            # Get the channel indices involved in stimulation\n",
    "            jacksheet = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Jacksheet']\n",
    "            if reref == 'CommonAverage':\n",
    "                stim_ix = []\n",
    "                stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Anode']))\n",
    "                stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Cathode']))    \n",
    "                stim_ix = np.unique([iyy for ixx in stim_ix for iyy in ixx])            \n",
    "            else:\n",
    "                raise Exception('reref currently supports CommonAverage')\n",
    "            nsstim_ix = np.setdiff1d(np.arange(len(jacksheet)), stim_ix)\n",
    "            \n",
    "            if not (key_dict['Subject_ID'] in sel_coh_base.Subject_ID.tolist()):\n",
    "                continue\n",
    "                \n",
    "            sel_base_subj = sel_coh_base.loc[sel_coh_base.Subject_ID == key_dict['Subject_ID']]\n",
    "            vals[s_ii, c_ii, 0] = sel_base_subj['Base_Node_Str'].iloc[0][:, stim_ix].mean()    \n",
    "            vals[s_ii, c_ii, 1] = np.nan_to_num(np.nanmean(del_behs))\n",
    "            \n",
    "            #for perm_ii in xrange(10000):\n",
    "            #    sel_nsstim_ix = np.random.permutation(nsstim_ix)[:len(stim_ix)]\n",
    "            #    null_vals[s_ii, c_ii, perm_ii] = sel_base_subj['Base_Node_Str'].iloc[0][:, sel_nsstim_ix].mean()    \n",
    "    \n",
    "    # Remove invalid observations\n",
    "    #null_vals = null_vals[np.unique(np.nonzero(~np.isnan(vals))[0]), :, :]        \n",
    "    vals = vals[np.unique(np.nonzero(~np.isnan(vals))[0]), :, :]\n",
    "\n",
    "    return vals, null_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Classifier States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Baseline to Memory Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key =['Subject_ID',\n",
    "                  'Stim_Anode',\n",
    "                  'Stim_Cathode']\n",
    "meta_stim_type = meta_ps.loc[meta_ps.Stim_Type == 'stimulating']\n",
    "meta_sham_type = meta_ps.loc[meta_ps.Stim_Type == 'sham']\n",
    "\n",
    "meta_stim_lut = meta_stim_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_sham_lut = meta_sham_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_base_lut = meta_base.groupby(['Subject_ID'])['Base_ID'].unique()\n",
    "\n",
    "meta_memr = meta_dict['memory_info'][reref]\n",
    "meta_memr['Delta_Prob'] = meta_memr['Post_Stim_Prob'] - meta_memr['Pre_Stim_Prob']\n",
    "meta_memr_lut = meta_memr.groupby(unique_stim_key)['Delta_Prob'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Node Strength with Average Memory Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def compute_baseline_memclass():\n",
    "    \n",
    "    # Initialize the measurement buckets\n",
    "    vals = {'stim': np.nan*np.zeros((len(meta_memr_lut), len(meta_dict['coherence_info']), 2)),\n",
    "            'null': np.nan*np.zeros((len(meta_memr_lut), len(meta_dict['coherence_info']), 10000))}\n",
    "    \n",
    "    # Iterate over coherence\n",
    "    for c_ii, coh_id in enumerate(meta_dict['coherence_info']):\n",
    "        sel_coh_base = df_globaltopo_base[df_globaltopo_base['Coherence_ID'] == coh_id]\n",
    "        \n",
    "        # Iterate over unique stim parameters\n",
    "        for s_ii, (meta_key, del_behs) in enumerate(meta_memr_lut.iteritems()):\n",
    "            key_dict = dict(zip(unique_stim_key, meta_key))\n",
    "              \n",
    "            # Get the channel indices involved in stimulation\n",
    "            jacksheet = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Jacksheet']\n",
    "            if reref == 'CommonAverage':\n",
    "                stim_ix = []\n",
    "                stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Anode']))\n",
    "                stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Cathode']))    \n",
    "                stim_ix = np.unique([iyy for ixx in stim_ix for iyy in ixx])            \n",
    "            else:\n",
    "                raise Exception('reref currently supports CommonAverage')\n",
    "            nsstim_ix = np.setdiff1d(np.arange(len(jacksheet)), stim_ix)\n",
    "            \n",
    "            if not (key_dict['Subject_ID'] in sel_coh_base.Subject_ID.tolist()):\n",
    "                continue\n",
    "\n",
    "            if (len(stim_ix) == 0) or (len(nsstim_ix) == 0):\n",
    "                continue\n",
    "                                \n",
    "            sel_base_subj = sel_coh_base.loc[sel_coh_base.Subject_ID == key_dict['Subject_ID']]\n",
    "            vals['stim'][s_ii, c_ii, 0] = sel_base_subj['Base_Node_Str'].iloc[0][:, stim_ix].mean()    \n",
    "            vals['stim'][s_ii, c_ii, 1] = np.nan_to_num(np.nanmean(del_behs))\n",
    "            \n",
    "            for perm_ii in xrange(10000):\n",
    "                sel_nsstim_ix = np.random.permutation(nsstim_ix)[:len(stim_ix)]\n",
    "                vals['null'][s_ii, c_ii, perm_ii] = sel_base_subj['Base_Node_Str'].iloc[0][:, sel_nsstim_ix].mean()\n",
    "\n",
    "    # Remove invalid observations\n",
    "    vals['null'] = vals['null'][np.unique(np.nonzero(~np.isnan(vals['stim']))[0]), :, :]        \n",
    "    vals['stim'] = vals['stim'][np.unique(np.nonzero(~np.isnan(vals['stim']))[0]), :, :]\n",
    "\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Comparative Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Node_Str_Delta':   {'xlabel': 'Frequency Range',\n",
    "                                 'ylabel': 'Corr. Baseline, Mem. Enc. State ',\n",
    "                                 'xlim': [-0.5, len(meta_dict['coherence_info'])-0.5],\n",
    "                                 'ylim': [0.0, 0.15],\n",
    "                                 'stim': {'color': [1.0, 0.3, 0.3]},\n",
    "                                 'null': {'color': [0.2, 0.2, 0.2]}}}\n",
    "n_grp = len(meta_dict['coherence_freq'])            \n",
    "\n",
    "for meas in analysis.keys():\n",
    "    vals = compute_baseline_memclass()\n",
    "        \n",
    "    all_corr = []\n",
    "    all_mean = []\n",
    "    all_serr = []\n",
    "    all_color = []\n",
    "    all_pos = []\n",
    "    all_pv = []\n",
    "    for c_ii in xrange(n_grp):\n",
    "        true_rho = stats.spearmanr(vals['stim'][:, c_ii, 0], vals['stim'][:, c_ii, 1])[0]\n",
    "        \n",
    "        null_rho = []\n",
    "        for n_ii in xrange(vals['null'].shape[2]):\n",
    "            null_rho.append(np.arctanh(stats.spearmanr(vals['null'][:, c_ii, n_ii], vals['stim'][:, c_ii, 1])[0]))\n",
    "        null_rho = np.array(null_rho)\n",
    "        \n",
    "        all_pv.append(np.mean(null_rho > true_rho))\n",
    "        \n",
    "        for corr in [[true_rho], null_rho]:\n",
    "            all_mean.append(np.nanmean(corr))\n",
    "            all_serr.append(np.nanstd(corr) / np.sqrt(len(corr)))\n",
    "            all_color.append(analysis[meas][tp]['color'])\n",
    "            all_pos.append(c_ii + 0.2*tp_ii)\n",
    "        \n",
    "    plt.figure(figsize=(2,1.5), dpi=300.0)\n",
    "    ax = plt.subplot(111)\n",
    "    \n",
    "    ax.bar(left=all_pos, height=all_mean, yerr=all_serr,\n",
    "           width=0.2, color=all_color, lw=0.0)\n",
    "    \n",
    "    ax.set_xlim(analysis[meas]['xlim'])\n",
    "    ax.set_ylim(analysis[meas]['ylim'])\n",
    "    ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "    ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "    ax.xaxis.set_ticks(np.arange(n_grp)+0.2)\n",
    "    ax.xaxis.set_ticklabels(meta_dict['coherence_freq'])\n",
    "    ax.locator_params(nbins=4, axis='y')    \n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    \n",
    "    #plt.savefig('{}/Evoked_Nodes-Func_Coherence.BarPlot.svg'.format(path_Data['Figure'][reref]))\n",
    "    #plt.close()\n",
    "\n",
    "    \"\"\"\n",
    "    for ff in xrange(len(meta_dict['coherence_freq'])):\n",
    "        write('{}\\n'.format(meta_dict['coherence_freq'][ff]))\n",
    "        write('t-test: {}\\n'.format(stats.ttest_rel(all_corr[0][:, ff], all_corr[1][:, ff])))\n",
    "        write('cohens: {}\\n\\n'.format(cohens_d(all_corr[0][:, ff], all_corr[1][:, ff])))\n",
    "    write('Anova: {}\\n'.format(stats.f_oneway(*all_corr[0].T)))\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Structural Control to Memory Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find unique stimulation parameter sets\n",
    "# Criteria: Subject_ID, Stim_Freq, Stim_Amp, Stim_Dur, Stim_Anode, Stim_Cathode\n",
    "unique_stim_key =['Subject_ID',\n",
    "                  'Stim_Anode',\n",
    "                  'Stim_Cathode']\n",
    "meta_stim_type = meta_ps.loc[meta_ps.Stim_Type == 'stimulating']\n",
    "meta_sham_type = meta_ps.loc[meta_ps.Stim_Type == 'sham']\n",
    "\n",
    "meta_stim_lut = meta_stim_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_sham_lut = meta_sham_type.groupby(unique_stim_key)['Event_ID'].unique()\n",
    "meta_base_lut = meta_base.groupby(['Subject_ID'])['Base_ID'].unique()\n",
    "\n",
    "meta_memr = meta_dict['memory_info'][reref]\n",
    "meta_memr['Delta_Prob'] = meta_memr['Post_Stim_Prob'] - meta_memr['Pre_Stim_Prob']\n",
    "meta_memr_lut = meta_memr.groupby(unique_stim_key)['Delta_Prob'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controllability with Average Memory Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#@memory.cache\n",
    "def compute_structural_memclass(loc_type):\n",
    "    \n",
    "    # Initialize the measurement buckets\n",
    "    vals = np.nan*np.zeros((len(meta_memr_lut), 2))\n",
    "    locs = {}\n",
    "            \n",
    "    # Iterate over unique stim parameters\n",
    "    for s_ii, (meta_key, del_behs) in enumerate(meta_memr_lut.iteritems()):\n",
    "        key_dict = dict(zip(unique_stim_key, meta_key))\n",
    "\n",
    "        # Get the channel indices involved in stimulation\n",
    "        jacksheet = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Jacksheet']\n",
    "        if reref == 'CommonAverage':\n",
    "            stim_ix = []\n",
    "            stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Anode']))\n",
    "            stim_ix.append(np.flatnonzero(jacksheet == key_dict['Stim_Cathode']))    \n",
    "            stim_ix = np.unique([iyy for ixx in stim_ix for iyy in ixx])            \n",
    "        else:\n",
    "            raise Exception('reref currently supports CommonAverage')\n",
    "        nsstim_ix = np.setdiff1d(np.arange(len(jacksheet)), stim_ix)\n",
    "\n",
    "        if (len(stim_ix) == 0) or (len(nsstim_ix) == 0):\n",
    "            continue\n",
    "            \n",
    "        if not (key_dict['Subject_ID'] in meta_dict['struct_adj'][reref].keys()):\n",
    "            continue\n",
    "\n",
    "        scale_id = 'scale125'\n",
    "        atlas_index = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Atlas_Index'][scale_id]\n",
    "        atlas_label = meta_dict['electrode_loc'][reref][key_dict['Subject_ID']][reref]['Atlas_Label'][scale_id]        \n",
    "        adj_struct = meta_dict['struct_adj'][reref][key_dict['Subject_ID']][scale_id]['QA']['adj']\n",
    "        \n",
    "        if loc_type == 'Struct_Degree':\n",
    "            struct_val = np.mean(adj_struct, axis=0)\n",
    "        \n",
    "        if loc_type == 'Struct_Avg_Cntl':\n",
    "            struct_val = structural_control.average_control(adj_struct)\n",
    "        \n",
    "        if loc_type == 'Struct_Mod_Cntl':\n",
    "            struct_val = structural_control.modal_control(adj_struct)            \n",
    "        \n",
    "        struct_val = (struct_val - struct_val.mean()) / struct_val.std()\n",
    "        \n",
    "        \n",
    "        not_LTC = True\n",
    "        for s_ix in stim_ix:\n",
    "            stim_lbl = atlas_label[s_ix]\n",
    "            if stim_lbl.split('_')[1] in meta_dict['LTC_info']:\n",
    "                not_LTC = False\n",
    "                if not stim_lbl in locs.keys():\n",
    "                    locs[stim_lbl] = []\n",
    "                locs[stim_lbl].append(struct_val[atlas_index][s_ix])\n",
    "        if not_LTC:\n",
    "            continue\n",
    "        \n",
    "        vals[s_ii, 0] = np.nan_to_num(np.nanmean(del_behs))\n",
    "        vals[s_ii, 1] = np.nanmean(struct_val[atlas_index][stim_ix])\n",
    "            \n",
    "    # Remove invalid observations\n",
    "    vals = vals[np.unique(np.nonzero(~np.isnan(vals))[0]), :]\n",
    "\n",
    "    return vals, locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Linear Regression for Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "analysis = {'Struct_Degree':     {'xlabel': 'Struc. Node Strength',\n",
    "                                  'ylabel': 'Mem. Enc. State',\n",
    "                                  'xlim': [0.0, 0.01],\n",
    "                                  'ylim': [-0.02, 0.02],\n",
    "                                  'color': [0.2, 0.2, 0.2]},\n",
    "            'Struct_Avg_Cntl':   {'xlabel': 'Average Controll.',\n",
    "                                  'ylabel': 'Mem. Enc. State',\n",
    "                                  'xlim': [1.00, 1.06],\n",
    "                                  'ylim': [-0.02, 0.02],\n",
    "                                  'color': [0.2, 0.2, 0.2]},\n",
    "            'Struct_Mod_Cntl':   {'xlabel': 'Modal Controllability\\n(z-score)',\n",
    "                                  'ylabel': 'Mem. Enc. State',\n",
    "                                  'xlim': [-1.5, 1.5],\n",
    "                                  'ylim': [-0.02, 0.02],                             \n",
    "                                  'color': [0.2, 0.2, 0.2]}}\n",
    "\n",
    "for meas in ['Struct_Mod_Cntl']: #analysis.keys():\n",
    "    vals, locs = compute_structural_memclass(meas)\n",
    "    \n",
    "    plt.figure(figsize=(1.5,4), dpi=300.0)\n",
    "    ax = plt.subplot(4,1,1)\n",
    "    ax.set_xlim(analysis[meas]['xlim'])\n",
    "    ax.set_ylim(analysis[meas]['ylim'])\n",
    "    ax.set_xlabel(analysis[meas]['xlabel'])\n",
    "    ax.set_ylabel(analysis[meas]['ylabel'])\n",
    "    ax.locator_params(nbins=3, axis='both')\n",
    "    #if meas == 'Struct_Avg_Cntl':\n",
    "    #    ax.locator_params(nbins=4, axis='x')\n",
    "    ax.set_xticks([-1.5, 0, 1.5])\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "    ax = sns.regplot(x=vals[:, 1], y=vals[:, 0],\n",
    "                     color=analysis[meas]['color'],\n",
    "                     scatter_kws={'s':1.0}, ci=68, ax=ax)\n",
    "\n",
    "    slp, yint, rv, pv, stdr = stats.linregress(vals[:, 1], vals[:, 0])\n",
    "    #rv, pv = stats.spearmanr(vals[:, 1], vals[:, 0])\n",
    "    write('{}: rv={:2.2f}, pv={:2.2e}\\n'.format(meas, rv, pv))\n",
    "    write('\\n')\n",
    "    \n",
    "    plt.savefig('{}/{}-Mem_Enc_State.LinReg.svg'.format(path_Data['Figure'][reref], meas))\n",
    "    plt.close()\n",
    "write('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modal Controllability Distribution (MNI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pop_modal_ctl = np.array([structural_control.modal_control(meta_dict['struct_adj']['CommonAverage'][subj_key]['scale125']['QA']['adj'])\n",
    "                          for subj_key in meta_dict['struct_adj']['CommonAverage'].keys()])\n",
    "\n",
    "pop_modal_ctl = pop_modal_ctl.mean(axis=0)\n",
    "pop_modal_ctl_zscored = pop_modal_ctl\n",
    "#pop_modal_ctl_zscored = (pop_modal_ctl - pop_modal_ctl.mean()) / pop_modal_ctl.std()\n",
    "\n",
    "\n",
    "itksnap_list = [[0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "norm = plt.Normalize(vmin=0.96, vmax=1.0)\n",
    "cmap = plt.get_cmap('Spectral_r')\n",
    "for ii, mm in enumerate(pop_modal_ctl_zscored):\n",
    "    rgba = cmap(norm(mm))\n",
    "    \n",
    "    itksnap_list.append([ii+1, int(255*rgba[0]), int(255*rgba[1]), int(255*rgba[2]), 1, 1, 1, ii+1])\n",
    "np.savetxt('/Users/akhambhati/Remotes/CORE.MRI_Atlases/Lausanne/modal.txt', itksnap_list, fmt='    %2d    %3d    %3d    %3d        %d  %d  %d    \\\"%s\\\"')\n",
    "    \n",
    "# IDX   -R-  -G-  -B-  -A--  VIS MSH  LABEL\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Energy Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x, y = np.mgrid[-1:1:.005, -1:1:.005]\n",
    "pos = np.empty(x.shape + (2,))\n",
    "pos[:, :, 0] = x; pos[:, :, 1] = y\n",
    "rv_1 = stats.multivariate_normal(mean=[-0.50, +0.50], cov=[0.04, 0.04])\n",
    "rv_2 = stats.multivariate_normal(mean=[+0.00, -0.50], cov=[0.25, 0.25])\n",
    "rv_3 = stats.multivariate_normal(mean=[+0.25, +0.75], cov=[0.10, 0.10])\n",
    "rv = 0.75*rv_1.pdf(pos) + 2.00*rv_2.pdf(pos) + rv_3.pdf(pos)\n",
    "rv = (rv - rv.min()) / (rv.max()-rv.min())\n",
    "\n",
    "fig = plt.figure(figsize=(3,3), dpi=300.0)\n",
    "ax = fig.gca(projection='3d')\n",
    "ss = ax.plot_surface(x, y, rv, cmap=plt.cm.YlGnBu_r, lw=0.0)\n",
    "ax.view_init(25, -40)\n",
    "ax.set_axis_off()\n",
    "plt.colorbar(ss, ax=ax)\n",
    "plt.savefig('{}/Artifical_Energy_Landscape.svg'.format(path_Data['Figure'][reref]))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "336px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_position": {
    "height": "836px",
    "left": "0px",
    "right": "1707px",
    "top": "107px",
    "width": "432px"
   },
   "toc_section_display": "block",
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "toc_position": {
   "height": "655px",
   "left": "1562.08px",
   "right": "20px",
   "top": "129px",
   "width": "340px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
